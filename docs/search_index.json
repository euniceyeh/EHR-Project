[
["index.html", "Electronic Health Records: Who Uses It? Chapter 1 Overview and Motivation", " Electronic Health Records: Who Uses It? Eunice Yeh, Lauren Yoo, Katherine Wang 2017-12-10 Chapter 1 Overview and Motivation This is the final project report for Fall 2017 Data Science I (BST 260) at Harvard T.H. Chan School of Public Health on the relationships between the use of Electronic Health Records (EHR) and provider demographics, separately among the eligible professionals (EPs) and eligible hospitals participating in the 2016 Medicare &amp; Medicaid EHR Incentive Program across the United States. Enjoy! "],
["work.html", "Chapter 2 Related Work", " Chapter 2 Related Work Anything that inspired you, such as a paper, a web site, or something we discussed in class. In the past 10 years, there has been a lot of discussion around the benefits and risks of adopting electronic health records (EHR’s). Many news sources have covered the topic, and multiple government agencies have published recommendations and articles on the adoption of EHR’s. From 2008 to 2015, the number of office-based practitioners with EHR systems doubled, proving that the popularity of EHR’s are only increasing (Healthit.gov). In 2011, the Centers for Medicare &amp; Medicaid Services (CMS) initiated the Medicare and Medicaid Electronic Health Record Incentive Programs to encourage medical professionals to adopt EHR technology. The program includes incentives for different stages of implementation, including establishing EHR requirements, demonstrating meaningful use, and a final stage to demonstrate improved health outcomes that will start this year (2017). The CMS provides millions of openly accessible data that they have used to evaluate the effectiveness of their incentive program. Looking through these data, we were inspired to utilize some of the R packages and tools that we have been learning in class, hoping to enable a better understanding of what types of providers enrolled in this incentive program are actually using EHR. Specifically, we wanted to implement the following course concepts: Data Wrangling Import and combine different sets of data in order to analyze relationships that have not been done before. Scrape data directly off of websites outside of those related to CMS in order to analyze more than what they already have. Visualization Apply the principles of data visualizations to accurately and meaningfully present our statistical findings. Create super cool while informative plots to help visualize our exploratory analyses. Statistical Analysis Perform regression analysis on the relationship between EHR use and provider demographics. Report and interpret statistical inference from our results. "],
["questions.html", "Chapter 3 Initial Questions", " Chapter 3 Initial Questions What questions are you trying to answer? How did these questions evolve over the course of the project? What new questions did you consider in the course of your analysis? Originally, we wanted to understand what kinds of people are more likely to adopt EHR in general. But then we learned about the Medicare &amp; Medicaid EHR Incentive Program that is run by the government to encourage providers to use EHR. We looked at hospital and physician level data from 2016 to see if there might be some similar characteristics of healthcare providers who are and are not adopting EHR use. Since EHR’s are a technology-based implementation, our group wanted to see if younger physicians are more likely to adopt EHR technology. What are some of the factors that might be associated with adoption of EHR’s? Additionally, we know that physicians working in hospitals do not have a choice in adopting EHR’s. As a result, we wanted to see if there were hospital-related factors affecting adoption of EHRs. To qualify for the incentive program, participants in the program need to use certified EHR technology as recognized by CMS. This can include a comprehensive package offered by one vendor or different modules or products offered by different vendors. All must be certified, but practitioners have some choice in implementation. As a result, we also thought it might be interesting to see how vendors and products are distributed among the EHR users. We also hoped to gain some insight to see if there were any differences in trends across the United States - are the factors correlated with adoption of EHR’s consistent across the country? Or are just a few states leading the pack? Since the Medicare &amp; Medicaid EHR Incentive Program accepts two types of providers in two different ways, our primary objective of looking at provider demographics on the use of EHR is split into two separate parts: What are the demographics of eligible physicians in the incentive program who have been using EHR; are these different from those who have yet to use EHR? What are the demographics of eligible hospitals in the incentive program who have been using EHR; are these different from those who have yet to use EHR? Our secondary objectives are then: Are all of these demographics and effects on EHR use different geographically? What are the demographics of both types of providers who use EHR by the specific EHR vendor or product types? "],
["data.html", "Chapter 4 Data 4.1 Eligible Professionals (EPs) 4.2 Eligible Hospitals (Hosp) 4.3 Vendors &amp; Products (EHR)", " Chapter 4 Data All of our data are based on the 2016 Medicare &amp; Medicaid EHR Incentive Program since the adoption of EHR is an ongoing basis: a provider who adopted and used EHR during the 2014 program, for example, is likely to still be in the program the following years. So theoretically, the 2016 program would be the most cumulative source of providers adopting EHR. As mentioned in Related Work, the adoption of EHR has been consistently rising over the years across the U.S., so there is also no interest in looking at the use over time when we can just look at the latest collection of data. And since 2017 isn’t quite over yet, it wouldn’t be fair to start looking at the usage of EHR within the 2017 program, so the most recent completed data would be on the 2016 program. 4.1 Eligible Professionals (EPs) To answer the first of our Initial Questions as to whether or not a physician’s age, education, practice location, etc. may be related to their use of EHR in the incentive program, we must be able to find demographics data at the practitioner level. 4.1.1 Source/Metadata And here we are: readily available on Data.Medicare.gov are the demographics of physicians eligible for the Medicare &amp; Medicaid EHR incentive program at the practitioner level. We used the API link to read in the data set and applied SQL filtering options to speed up the download since it is a large file. We specified the limit to be exactly 2254703 because that’s the exact number of rows the data contain, if we don’t specify this, the default limit of rows that get downloaded is 100. To compensate, we will specify the exact columns of interest to include, which can also be specified in the download link (we looked through the metadata on the website to select the meaningful and interesting variables). dat &lt;- read.csv(&quot;https://data.medicare.gov/resource/c8qv-268j.csv?$limit=2254703&amp;$select=npi,frst_nm,gndr,cred,med_sch,grd_yr,pri_spec,cty,st,zip,hosp_afl_1,hosp_afl_lbn_1,ehr&amp;$order=npi&quot;) 4.1.2 Wrangling Let’s first take a look at the data structure and a summary of all the columns. str(dat) ## &#39;data.frame&#39;: 2253109 obs. of 13 variables: ## $ cred : Factor w/ 22 levels &quot;&quot;,&quot;AA&quot;,&quot;AU&quot;,&quot;CNA&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ## $ cty : Factor w/ 10939 levels &quot;20850&quot;,&quot;ABBEVILLE&quot;,..: 2504 292 8513 2982 9755 1103 5955 5955 387 387 ... ## $ ehr : Factor w/ 2 levels &quot;&quot;,&quot;Y&quot;: 1 1 1 1 2 1 1 1 1 1 ... ## $ frst_nm : Factor w/ 70256 levels &quot;A&quot;,&quot;A&#39;LANNE&quot;,..: 4301 4301 4301 62612 50322 13029 26683 26683 31162 31162 ... ## $ gndr : Factor w/ 3 levels &quot;F&quot;,&quot;M&quot;,&quot;U&quot;: 2 2 2 2 2 2 1 1 2 2 ... ## $ grd_yr : int 1994 1994 1994 2003 1999 2003 2007 2007 1997 1997 ... ## $ hosp_afl_1 : Factor w/ 4758 levels &quot;&quot;,&quot;010001&quot;,&quot;010005&quot;,..: 4382 4382 4382 1115 3226 3625 3174 3174 623 623 ... ## $ hosp_afl_lbn_1: Factor w/ 4527 levels &quot;&quot;,&quot;ABBEVILLE AREA MEDICAL CENTER&quot;,..: 2028 2028 2028 1130 2413 2975 1950 1950 4214 4214 ... ## $ med_sch : Factor w/ 406 levels &quot;A T STILL UN, ARIZONA SCHL OF DENT.Y &amp; ORAL HLTH&quot;,..: 220 220 220 327 220 228 267 267 214 214 ... ## $ npi : int 1003000126 1003000126 1003000126 1003000134 1003000142 1003000407 1003000423 1003000423 1003000480 1003000480 ... ## $ pri_spec : Factor w/ 82 levels &quot;&quot;,&quot;ADDICTION MEDICINE&quot;,..: 38 38 38 60 5 24 51 51 27 27 ... ## $ st : Factor w/ 55 levels &quot;AK&quot;,&quot;AL&quot;,&quot;AR&quot;,..: 49 49 49 16 38 41 38 38 6 6 ... ## $ zip : Factor w/ 227946 levels &quot;000075949&quot;,&quot;00079&quot;,..: 52803 58388 61477 141909 108444 43289 109249 109249 181915 181915 ... summary(dat) ## cred cty ehr frst_nm ## :1496066 HOUSTON : 37772 :1763471 MICHAEL: 45069 ## MD : 507108 PITTSBURGH : 37209 Y: 489638 DAVID : 40803 ## PA : 43256 NEW YORK : 27363 JOHN : 39636 ## NP : 38252 PHILADELPHIA: 26357 ROBERT : 32685 ## DO : 31818 CHICAGO : 24889 JAMES : 29921 ## CNA : 24669 ANN ARBOR : 23870 (Other):2064973 ## (Other): 111940 (Other) :2075649 NA&#39;s : 22 ## gndr grd_yr hosp_afl_1 ## F:1006865 Min. :1939 : 663101 ## M:1246243 1st Qu.:1988 230046 : 20177 ## U: 1 Median :1999 450076 : 10417 ## Mean :1997 490009 : 8558 ## 3rd Qu.:2008 200033 : 8192 ## Max. :2017 390111 : 8028 ## NA&#39;s :5852 (Other):1534636 ## hosp_afl_lbn_1 ## : 664977 ## UNIVERSITY OF MICHIGAN HEALTH SYSTEM : 20177 ## UNIVERSITY OF TEXAS M D ANDERSON CANCER CENTER,THE: 10417 ## UNIVERSITY OF VIRGINIA MEDICAL CENTER : 8558 ## EASTERN MAINE MEDICAL CENTER : 8192 ## HOSPITAL OF UNIV OF PENNSYLVANIA : 8028 ## (Other) :1532760 ## med_sch ## OTHER :1139579 ## WAYNE STATE UNIVERSITY SCHOOL OF MEDICINE : 17127 ## INDIANA UNIVERSITY SCHOOL OF MEDICINE : 16092 ## PHILADELPHIA COLLEGE OF OSTEOPATHIC MEDICINE: 14626 ## TEMPLE UNIVERSITY SCHOOL OF MEDICINE : 14045 ## UNIVERSITY OF MICHIGAN MEDICAL SCHOOL : 13798 ## (Other) :1037842 ## npi pri_spec st ## Min. :1.003e+09 NURSE PRACTITIONER : 220488 CA : 194572 ## 1st Qu.:1.246e+09 INTERNAL MEDICINE : 206760 TX : 166368 ## Median :1.498e+09 FAMILY PRACTICE : 175076 PA : 148393 ## Mean :1.500e+09 PHYSICIAN ASSISTANT : 166506 NY : 139718 ## 3rd Qu.:1.741e+09 PHYSICAL THERAPY : 103461 FL : 123617 ## Max. :1.993e+09 DIAGNOSTIC RADIOLOGY: 94251 MI : 99338 ## (Other) :1286567 (Other):1381103 ## zip ## 481095000: 21240 ## 152124756: 9758 ## 770304000: 8983 ## 229080001: 7270 ## 326103003: 6256 ## 110303816: 5614 ## (Other) :2193988 Observations from the above output: There are 53 levels of states, which means this data include states outside of the 50 mainland states and DC. we don’t have sufficient data on the virgin islands and Guam, so we will have to filter these states out. There is one person whose gender is unknown, we will filter this person out since we want to group by gender. There are rows with missing graduation years, will need to filer this out too if graduation years is one of the main demographics of interest (will mess up plotting). Also, one or more physician who graduated in 2017 slipped into our data…we will ignore this by filtering it out too. The zip code, 1, is out of range. a standard zip code has to be at least 5 digits. looks like most zip codes in this data are the full 9-digit zip codes, which can be split into the standard 5-digit format and the 4-digit after. There are way too many levels of medical schools and primary specialties, we will probably focus on looking at the most popular ones for exploratory analysis. data(&quot;state&quot;) clean &lt;- dat %&gt;% filter(st %in% c(state.abb, &quot;DC&quot;)) %&gt;% mutate(hosp_afl_1 = as.integer(hosp_afl_1)) %&gt;% filter(gndr %in% c(&quot;F&quot;, &quot;M&quot;) &amp; !is.na(as.integer(grd_yr)) &amp; grd_yr != 2017) Now we only have 51 levels of states, 2 levels of gender, and no records with missing graduation year. But we still have zip codes as small as 1, which is still out of range. To make sure only keep valid zip codes (since there may be other ways the zip codes are not clean), we can try to use R’s zip code package to validate our zip codes. # separate out the first five zip codes from the last four extension to match R zipcode data clean &lt;- clean %&gt;% mutate(zip.ext = substr(as.character(zip), 6, 9), zip = substr(as.character(zip), 1, 5)) # use R zipcode package and data library(zipcode) data(&quot;zipcode&quot;) zip_dat &lt;- clean %&gt;% left_join(zipcode,by=&#39;zip&#39;) # check if any of the invalid zip codes were matched to R&#39;s zip code package zip_dat %&gt;% filter(nchar(zip) &lt; 5) %&gt;% select(npi, frst_nm, zip, cty, st, city, state) %&gt;% head(15) ## [1] npi frst_nm zip cty st city state ## &lt;0 rows&gt; (or 0-length row.names) As expected, none of the zip codes with only 4 digits matched, so these 0 records will have to be thrown away in order for us to be able to accurately do analysis on the location variables and regional analysis and to match with the vendor data by zip code. zip_mismatch &lt;- zip_dat %&gt;% mutate(st = as.character(st)) %&gt;% filter(st != state | nchar(zip) &lt; 5) %&gt;% select(npi, frst_nm, zip, cty, city, st, state, latitude, longitude) # check number of records mismatched nrow(zip_mismatch) ## [1] 1573 # look at the top five mismatches (top5_mismatch &lt;- zip_mismatch %&gt;% group_by(zip, cty, st, city, state) %&gt;% summarize(n = n()) %&gt;% arrange(desc(n)) %&gt;% head(5)) ## # A tibble: 5 x 6 ## # Groups: zip, cty, st, city [5] ## zip cty st city state n ## &lt;chr&gt; &lt;fctr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 99362 WALLA WALLA WA Walla Walla OR 336 ## 2 52761 MUSCATINE IA Muscatine IL 171 ## 3 22401 FREDERICKSBURG MD Fredericksburg VA 129 ## 4 43530 GRANGER IN Jewell OH 128 ## 5 20176 HANOVER MD Leesburg VA 106 In order to not just blatantly throw away 1573 mismatches, we will look at the top 5 most common mismatches on a case-by-case basis to try to salvage as many records as possible without searching too deeply in our data set. Also, starting from the 6th most common mismatches, the number of records n were down to 58 or less, so they would have little effect on the data set. The results of searching up the top 10 mismatches on Google Maps: The zip code 99362 spans across Walla Walla in both WA and OR, so it is likely that the practice is located on the WA side of the zip code coverage. We will ignore the state mismatch from the zip code package for this case. The zip code 52761 does belong to Muscatine, IA, which is right on the border of IL. Again, the practice is most likely located on the IA side. Will ignore the state mismatch for this case as well. The zip code for Granger, IN was most likely mistyped. The correct zip code for Granger is 46530, which is very close to 43530. We will fix the zip code for this particular case only. The zip code 22401 does belong to Frederickburg, VA, and is not near the border of VA and MD at all. There is a “Frederick” city in MD, but none of its zip codes look similar enough to 22401. No clear culprit, will have to let this one go. :( The zip code for Hanover, MD is 21076, which was most likely mistyped as 20176. We will fix this particular case as well. # leave out the 22401 Fredericksburg MD mismatch, fix the zip codes for Granger and Hanover. zip_fix &lt;- top5_mismatch %&gt;% ungroup() %&gt;% filter(cty != &#39;FREDERICKSBURG&#39;) %&gt;% mutate(zip = replace(zip, zip %in% c(43530, 20176), c(46530, 21076))) %&gt;% select(-n) mismatch_tokeep &lt;- zip_dat %&gt;% select(-zip) %&gt;% # we want to replace the original wrong zip codes with the fixed zip codes inner_join(zip_fix, by = c(&#39;cty&#39;,&#39;st&#39;, &#39;city&#39;, &#39;state&#39;)) ## Warning: Column `st` joining factor and character vector, coercing into ## character vector clean &lt;- zip_dat %&gt;% mutate(st = as.character(st), state = as.character(state)) %&gt;% filter(st == state | nchar(zip) == 5) %&gt;% # keeping only the matches bind_rows(mismatch_tokeep) %&gt;% # then add in the mismatches we wanted to keep or have fixed select(-city, -state) # will rely on the original city and state columns Continuing on with our wrangling… About 0% of the (original) data are of physicians not affiliated with any hospital, these are the practitioners we will subset on to do our practitioner-level analysis on since these are the only cases when a physician’s demographics can have an effect on the choice of using EHR. For physicians who are affiliated with a hospital, their demographics would no longer be informative as to whether or not they use EHR, it would then have to be the demographics of the hospitals that could inform the use of EHR at the hospital-level. Thus, we were able to find hospital demographics such as ___ on hospitals across the U.S. in general, which will be discussed in the next section within this chapter, but there’s no indication of participation in the incentive program nor EHR use, so we will have to merge that hospital demographic data with the practitioners in this data set who are affiliated with any one hospital. for the practitioner-level analysis we want to: Calculate the number of years since graduation Use 1’s and 0’s instead of ‘Y’ and blanks as indicator of EHR use for our binary outcome Keep only one record per physician, which means we would have to lose the distinct practice locations for physicians who have multiple practices. But since we can’t fit our logistic model with either city, state, or zip code, there’s no point in keeping this detail. Instead, we can calculate the number of distinct practices these physicians have in order to collapse the data to the practitioner level. EPs &lt;- clean %&gt;% filter(is.na(as.integer(hosp_afl_1))) %&gt;% mutate(yrs_grd = 2016 - as.integer(grd_yr), # calc the n of years since graduation cred = case_when(cred == &#39;&#39; ~ &#39;NA&#39;, # assign blanks to NA&#39;s TRUE ~ as.character(cred)), cred = as.factor(cred), ehr = case_when(ehr == &#39;Y&#39; ~ 1, # use numeric indicators ehr == &#39;&#39; ~ 0), ehr = as.factor(ehr)) %&gt;% select(-hosp_afl_1, -hosp_afl_lbn_1, -frst_nm) %&gt;% distinct(npi, .keep_all = TRUE) # keep only one row per physician # calculate the number of practices for each physician SL &lt;- dat %&gt;% group_by(npi) %&gt;% summarise(locations = n_distinct(zip)) EPs &lt;- left_join(EPs, SL, by=&quot;npi&quot;) rm(SL) summary(EPs) ## cred cty ehr gndr grd_yr ## NULL: 20850 :0 NULL: F:0 Min. : NA ## ABBEVILLE :0 M:0 1st Qu.: NA ## ABBOTSFORD:0 U:0 Median : NA ## ABERDEEN :0 Mean :NaN ## ABERNATHY :0 3rd Qu.: NA ## ABILENE :0 Max. : NA ## (Other) :0 ## med_sch ## A T STILL UN, ARIZONA SCHL OF DENT.Y &amp; ORAL HLTH :0 ## ADIO/PENNSYLVANIA INSTITUTE OF STRAIGHT CHIROPRACTIC :0 ## ALBANY MEDICAL COLLEGE OF UNION UNIVERSITY :0 ## ALBERT EINSTEIN COLLEGE OF MEDICINE OF YESHIVA UNIVERSITY :0 ## AMERICAN MEDICAL MISSIONARY COLLEGE :0 ## ARIZONA COLLEGE OF OSTEOPATHIC MEDICINE MID WESTERN UNIVERSITY:0 ## (Other) :0 ## npi pri_spec ## Min. : NA :0 ## 1st Qu.: NA ADDICTION MEDICINE :0 ## Median : NA ADVANCED HEART FAILURE AND TRANSPLANT CARDIOLOGY:0 ## Mean :NaN ALLERGY/IMMUNOLOGY :0 ## 3rd Qu.: NA ANESTHESIOLOGY :0 ## Max. : NA ANESTHESIOLOGY ASSISTANT :0 ## (Other) :0 ## st zip zip.ext latitude ## Length:0 Length:0 Length:0 Min. : NA ## Class :character Class :character Class :character 1st Qu.: NA ## Mode :character Mode :character Mode :character Median : NA ## Mean :NaN ## 3rd Qu.: NA ## Max. : NA ## ## longitude yrs_grd locations ## Min. : NA Min. : NA Min. : NA ## 1st Qu.: NA 1st Qu.: NA 1st Qu.: NA ## Median : NA Median : NA Median : NA ## Mean :NaN Mean :NaN Mean :NaN ## 3rd Qu.: NA 3rd Qu.: NA 3rd Qu.: NA ## Max. : NA Max. : NA Max. : NA ## 4.1.3 Output # permanently save physician-level analysis dataset to repo saveRDS(EPs, file = &quot;./data/EPs.rds&quot;) # for the rest of practitioners who are affiliated with a hospital hosp_afl &lt;- clean %&gt;% # filter on the clean version filter(!is.na(as.integer(hosp_afl_1))) %&gt;% mutate(hosp_afl_lbn_1 = as.character(hosp_afl_lbn_1), cty = str_trim(as.character(cty)), st = as.character(st), ehr = ifelse(ehr == &quot;Y&quot;, 1, 0)) %&gt;% # only keep relevant or potentially interesting variables (keeping npi to count number of physicians) distinct(npi, hosp_afl_lbn_1, st, cty, ehr, hosp_afl_1, pri_spec, gndr, grd_yr) # clean up intermediate data sets rm(dat, clean, zipcode, zip_dat, zip_fix, zip_mismatch, mismatch_tokeep, top5_mismatch) We are storing the EPs data set as a .rds file in our repo under the /data directory for reproducibility of our analysis since this is the clean analysis-ready data set that will be used to analyze the potential effects of practitioner demographics on EHR use in our Primary Analysis. The hosp_afl data set, still containing one row per physician per hospital location, is to be merged with the hospital demographics data discussed in the next section. Both EPs and the merged-to-be data of hosp_afl containing the hospital demographics are to be merged with the EHR vendor and product information in the last section of this chapter, in order to explore both physician and hospital demographics by specific types of EHR used. 4.2 Eligible Hospitals (Hosp) General hospital demographics such as the total number of staffed beds, total number of discharges, total number of patient days, and total gross patient revenue (inpatient and outpatient) are all publicly accessible on the American Hospital Directory organized by U.S. states. Since there is not a .csv file ready for us to download and the hospital demographics are already neatly summarized in a table for each state, we decided that we could do some web scraping for this. 4.2.1 Source/Metadata The link to each U.S. state’s summary table of hospital demographics by hospital name in that state begins with the same URL: https://www.ahd.com/states/hospital_ and ends with the state’s abbreviation plus a .html. For example, a table of hospital demographics for each hospital in Massachusetts can be directly scraped from the link, https://www.ahd.com/states/hospital_MA.html. The details of these hospital statistics are described here. hosp_web &lt;- data.frame() for (st in c(state.abb, &quot;DC&quot;)){ url = paste(&quot;https://www.ahd.com/states/hospital_&quot;, st, &quot;.html&quot;, sep=&#39;&#39;) st_df &lt;- read_html(url) %&gt;% html_nodes(&quot;table&quot;) %&gt;% .[[2]] %&gt;% html_table() st_df &lt;- as.data.frame.matrix(eval(parse(text=st))) # removing the TOTAL row at the end of each table st_df &lt;- st_df[!(st_df$&quot;Hospital Name&quot;==&quot;T O T A L&quot;),] st_df$State &lt;- st hosp_web &lt;- rbind(hosp_web, st_df) } 4.2.2 Wrangling Let’s take a look at the data structure of the raw scrapped hospital demographics. str(hosp_web) ## &#39;data.frame&#39;: 3907 obs. of 7 variables: ## $ Hospital_name : chr &quot;Alaska Native Medical Center&quot; &quot;Alaska Regional Hospital&quot; &quot;Bartlett Regional Hospital&quot; &quot;Central Peninsula General Hospital&quot; ... ## $ City : chr &quot;Anchorage&quot; &quot;Anchorage&quot; &quot;Juneau&quot; &quot;Soldotna&quot; ... ## $ Staffed_beds : chr &quot;152&quot; &quot;174&quot; &quot;57&quot; &quot;106&quot; ... ## $ Total_discharges : chr &quot;7,320&quot; &quot;6,405&quot; &quot;1,710&quot; &quot;2,575&quot; ... ## $ Patient_days : chr &quot;38,687&quot; &quot;33,507&quot; &quot;6,325&quot; &quot;10,186&quot; ... ## $ Gross_patient_revenue: chr &quot;$0&quot; &quot;$947,754&quot; &quot;$147,231&quot; &quot;$305,469&quot; ... ## $ State : chr &quot;AK&quot; &quot;AK&quot; &quot;AK&quot; &quot;AK&quot; ... To do: We need to turn all the demographic information into numeric columns so we can use them as continuous variables. These values are reported with commas, so we need to strip these out. All the gross patient revenues are reported with a dollar sign in addition to the commas, so we need to strip those out too. Proper cases are harder to match on, will convert all letters in city and hospital name to upper case. It looks like some hospital has a gross patient revenue of $0…is this possible? Will explore this after dealing with all of the above first. hosp_clean &lt;- transform(hosp_web, Staffed_beds = as.numeric(gsub(&quot;,&quot;, &quot;&quot;, Staffed_beds)), Total_discharges = as.numeric(gsub(&quot;,&quot;, &quot;&quot;, Total_discharges)), Patient_days = as.numeric(gsub(&quot;,&quot;,&quot;&quot;, Patient_days)), Gross_patient_revenue = as.numeric(gsub(&quot;\\\\$|,&quot;, &quot;&quot;, Gross_patient_revenue)), Hospital_name = toupper(Hospital_name), City = toupper(str_trim(as.character(City))) ) summary(hosp_clean) ## Hospital_name City Staffed_beds Total_discharges ## Length:3907 Length:3907 Min. : 0.0 Min. : 0 ## Class :character Class :character 1st Qu.: 48.0 1st Qu.: 1244 ## Mode :character Mode :character Median : 133.0 Median : 4562 ## Mean : 204.8 Mean : 8017 ## 3rd Qu.: 264.0 3rd Qu.: 11336 ## Max. :36985.0 Max. :136646 ## Patient_days Gross_patient_revenue State ## Min. : 0 Min. : 0 Length:3907 ## 1st Qu.: 4104 1st Qu.: 98340 Class :character ## Median : 18892 Median : 377425 Mode :character ## Mean : 37794 Mean : 797958 ## 3rd Qu.: 51621 3rd Qu.: 1001421 ## Max. :695643 Max. :15618749 It looks like the zero value wasn’t just for the gross patient revenue. From the summary output, we can see that the minimum values on all hospital demographics (staffed beds, total discharges, patient days, gross patient revenue) are zero, which doesn’t seem right. Theoretically, it is impossible for any functioning hospital to have a total of zero on any of these characteristics. Would having a total of zero number of staffed beds really make it a hospital? Can a hospital survive on a gross patient revenue of zero dollars? We thought that it might be a web scrapping error, but when we went back and looked on the website, some of their tables definitely have random zero hospital statistics and some even have a whole row of zeros for certain hospitals. The metadata wasn’t very clear on this, but it seems like it is their way of representing missing data. Instead of keeping them as zeros, which would affect any computation we want to perform on these continuous variables, we will convert every zero into a true missing value, NA. But if a hospital has all missing demographics (the entire row of zeros), then we will just remove that hospital entirely (delete the whole row). # the demographics are columns 3 to 6 of the data frame is.na(hosp_clean[,3:6]) &lt;- !hosp_clean[,3:6] # turn 0&#39;s to NAs hosp_clean &lt;- hosp_clean[rowSums(is.na(hosp_clean)) &lt; 4,] # only delete hosp with all four NAs summary(hosp_clean) ## Hospital_name City Staffed_beds Total_discharges ## Length:3456 Length:3456 Min. : 1.0 Min. : 1 ## Class :character Class :character 1st Qu.: 73.0 1st Qu.: 2141 ## Mode :character Mode :character Median : 157.0 Median : 5692 ## Mean : 231.6 Mean : 9087 ## 3rd Qu.: 290.0 3rd Qu.: 12564 ## Max. :36985.0 Max. :136646 ## NA&#39;s :1 NA&#39;s :9 ## Patient_days Gross_patient_revenue State ## Min. : 1 Min. : 2298 Length:3456 ## 1st Qu.: 7740 1st Qu.: 188177 Class :character ## Median : 23922 Median : 504646 Mode :character ## Mean : 42838 Mean : 923192 ## 3rd Qu.: 57391 3rd Qu.: 1149258 ## Max. :695643 Max. :15618749 ## NA&#39;s :9 NA&#39;s :79 Now the minimum value is 1…still questionable and potentially causing the skew in their distributions. But we did take a look at these specific hospitals with any one demographic characteristic of value 1 and they seem to also have low values of the other characteristics as well (not necessarily exactly of 1, that would’ve been even more suspicious), so these may very well be valid who knows. Anyway, we need to move on with our lives, I mean, to merging with hosp_afl (which is a subset of practitioners who are affiliated with a hospital from the previous section) on the hospital name and state since different hospitals in different locations can have the same name. Since hosp_afl can have multiple rows per hospital if more than one physician in the data is affiliated with it, so we will aggregate the data back to hospital-level after the merge by summarizing physician-level information. hosps &lt;- inner_join(hosp_clean, hosp_afl, by=c(&quot;Hospital_name&quot;=&quot;hosp_afl_lbn_1&quot;, &quot;State&quot;=&quot;st&quot;)) hosps &lt;- hosps %&gt;% group_by(hosp_afl_1, Hospital_name, State, Staffed_beds, Total_discharges, Patient_days, Gross_patient_revenue) %&gt;% summarize(num_phys = n_distinct(npi), female_prop = round(mean(ifelse(gndr == &quot;F&quot;, 1, 0)),2), avg_grad_year = round(mean(grd_yr),2), n_specialty =n_distinct(pri_spec), EHR_use = as.factor(max(ehr))) summary(hosps) ## hosp_afl_1 Hospital_name State Staffed_beds ## Min. : 10001 Length:712 Length:712 Min. : 7.0 ## 1st Qu.:100311 Class :character Class :character 1st Qu.: 141.5 ## Median :230273 Mode :character Mode :character Median : 242.5 ## Mean :248324 Mean : 295.2 ## 3rd Qu.:370018 3rd Qu.: 380.5 ## Max. :670073 Max. :1750.0 ## ## Total_discharges Patient_days Gross_patient_revenue num_phys ## Min. : 12 Min. : 37 Min. : 3736 Min. : 1.000 ## 1st Qu.: 5101 1st Qu.: 20188 1st Qu.: 456340 1st Qu.: 1.000 ## Median :10860 Median : 46707 Median : 923908 Median : 1.000 ## Mean :13194 Mean : 63369 Mean : 1347802 Mean : 1.784 ## 3rd Qu.:17680 3rd Qu.: 85566 3rd Qu.: 1718996 3rd Qu.: 2.000 ## Max. :77757 Max. :425114 Max. :14140692 Max. :12.000 ## NA&#39;s :5 ## female_prop avg_grad_year n_specialty EHR_use ## Min. :0.000 Min. :1965 Min. :1.000 1:199 ## 1st Qu.:0.000 1st Qu.:2004 1st Qu.:1.000 0:513 ## Median :0.500 Median :2008 Median :1.000 ## Mean :0.512 Mean :2007 Mean :1.662 ## 3rd Qu.:1.000 3rd Qu.:2011 3rd Qu.:2.000 ## Max. :1.000 Max. :2016 Max. :8.000 ## table(hosps[which(hosps$num_phys &lt;= 10),]$EHR_use) ## ## 1 0 ## 198 513 Well, we managed to get about {r} (nrow(hosps)/nrow(hosp_clean))*100% match on the hospitals between the two data sets and the summary statistics on those hospital demographics look a lot nicer and less skewed! &lt;——actually check this again on final run!!!! Even though we aggregated the physical-level demographics because we didn’t want to just throw away data, we do acknowledge that these physician-level demographics are not really meaningful because they are not representative of the demographics of the hospital. It is unlikely that we had all the physicians information from our hosp_afl for each hospital that matched. (i.e. the 10 physicians we have data on for a hospital may not accurately represent the all practitioners in that hospital) We thought of excluding hospitals with less than 10 practitioners in the data to obtain accurate physician demographics for the hospital, but it would mean cutting down way to much of our limited data. We decided to just focus on the main hospital demographics scraped from the website, and keep the aggregated physician demographics at the exploratory level. Merging with hosp_afl still provided us the main outcome of interest, “EHR_use (the hospital uses the electronic health system)”, calculated as 1 if at least one practitioner in the hospital uses EHR and 0 if none of the practitioners in the hospital uses EHR - that’s why we took the maximum in our code above. Reminder that for practitioners affiliated with hospitals, we assumed that EHR use is the hospital-level adoption and not individual’s. Thus it makes sense that if at least one of the practitioners is recorded in the data as using EHR, we will assume the hospital uses EHR. 4.2.3 Output # clean up intermediate data sets rm(hosp_web, hosp_afl) # hosp_clean will be used later in secondary regional analysis # permanently save hospital-level analysis dataset to repo saveRDS(hosps, file = &quot;./data/hosps.rds&quot;) Again, we are storing the hosps data set as a .rds file in our repo under the /data directory for reproducibility of our analysis since it will be used to analyze the potential effects of hospital demographics on the use of EHR, which is another aspect of our Primary Analysis. This data set will also be subsequently merge with the EHR vendor and product information discussed in the next section to explore hospital demographics by specific types of EHR used. 4.3 Vendors &amp; Products (EHR) Since so many professionals and hospitals across the U.S. have already adopted EHR, we realized that simply looking at the binary outcome of EHR use would be too boring of a project. Thus, we were curious to further explore the demographics of those who do use EHR by the specific types of EHR vendor or product. Fortunately, we were able to find data to support this additional secondary/exploratory analysis. 4.3.1 Source/Metadata The Health IT Dashboard provides certified health IT product data from the ONC Certified Health IT Product List (CHPL) such as the unique vendors, products, and product types of each certified health IT product used as part of the Medicare EHR Incentive Program. We downloaded only the 2016 data set, which also includes unique provider identifiers (NPI), in order to match the EPs data set discussed in the first section of this chapter. As the metadata explains, a provider in this data set can be either an eligible professional (EP) and eligible hospital (Hospital), as distinguished by the Provider_Type column. Thus, only the Provider_Type == 'EP' records is merged with the subset of EPs who are not affiliated with any hospital, and the Provider_Type == 'Hospital' records is merged with the subset of EPs who are affiliated with a hospital, and therefore directly merged with the combined and cleaned version of the hosp data set discussed in the previous section of this chapter. EHR &lt;- read.csv(&quot;https://dashboard.healthit.gov/datadashboard/data/MU_REPORT_2016.csv&quot;) 4.3.2 Wrangling Data structure, you know the drill. str(EHR) ## &#39;data.frame&#39;: 505914 obs. of 23 variables: ## $ NPI : int 1003000142 1003000522 1003000597 1003000639 1003000902 1003000936 1003000936 1003001256 1003001256 1003001462 ... ## $ CCN : int NA NA NA NA NA NA NA NA NA NA ... ## $ Provider_Type : Factor w/ 2 levels &quot;EP&quot;,&quot;Hospital&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ Business_State_Territory : Factor w/ 56 levels &quot;Alabama&quot;,&quot;Alaska&quot;,..: 38 11 39 5 17 45 45 6 6 12 ... ## $ ZIP : Factor w/ 11921 levels &quot;&quot;,&quot;00603&quot;,&quot;00612&quot;,..: 5619 4304 9168 10670 5353 3731 3731 10037 10037 4055 ... ## $ Specialty : Factor w/ 288 levels &quot;&quot;,&quot;Acupuncturist&quot;,..: 177 9 279 263 79 27 27 79 79 235 ... ## $ Hospital_Type : Factor w/ 3 levels &quot;&quot;,&quot;Critical Access&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ## $ Program_Type : Factor w/ 2 levels &quot;Medicare&quot;,&quot;Medicare/Medicaid&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ Program_Year : int 2016 2016 2016 2016 2016 2016 2016 2016 2016 2016 ... ## $ Provider_Stage_Number : Factor w/ 2 levels &quot;Stage 1&quot;,&quot;Stage 2&quot;: 2 2 1 1 2 2 2 2 2 1 ... ## $ Payment_Year : int 3 5 NA NA 5 3 3 5 5 NA ... ## $ Attestation_Month : int 2 1 1 2 1 5 5 2 2 3 ... ## $ Attestation_Year : int 2017 2017 2017 2017 2017 2017 2017 2017 2017 2017 ... ## $ MU_Definition_2014 : logi NA NA NA NA NA NA ... ## $ Stage_2_Scheduled_2014 : int 0 1 0 0 1 0 0 0 0 0 ... ## $ EHR_Certification_Number : Factor w/ 3550 levels &quot;0014E0087U0WV35&quot;,..: 2699 1660 2177 2303 2965 1119 1119 2791 2791 2023 ... ## $ EHR_Product_CHP_Id : Factor w/ 2317 levels &quot;14.02.02.1026.A033.01.01.0.170217&quot;,..: 1876 346 1168 1403 2226 136 1747 1403 1876 1004 ... ## $ Vendor_Name : Factor w/ 484 levels &quot;4medica, Inc.&quot;,..: 134 35 338 134 159 170 170 134 134 120 ... ## $ EHR_Product_Name : Factor w/ 798 levels &quot;1 Connect BuildYourEMR&quot;,..: 192 42 554 192 81 251 252 192 192 533 ... ## $ EHR_Product_Version : Factor w/ 699 levels &quot;1&quot;,&quot;1.0&quot;,&quot;1.0.0&quot;,..: 489 113 385 494 50 549 206 494 489 155 ... ## $ Product_Classification : Factor w/ 3 levels &quot;&quot;,&quot;Complete EHR&quot;,..: 2 2 2 2 2 2 2 2 2 2 ... ## $ Product_Setting : Factor w/ 3 levels &quot;&quot;,&quot;Ambulatory&quot;,..: 2 2 2 2 2 2 2 2 2 2 ... ## $ Product_Certification_Edition_Yr: int 2014 2014 2014 2014 2014 2014 2014 2014 2014 2014 ... There are actually a lot of variables here that are not informative so we can drop them and just focus on a smaller set, i.e. program year would just be 2016 on every row in the data, EHR certification number or CHP ID is at an unnecessary level of detail. Let’s select the important columns and a few potentially interesting variables to summarize by (thus, using distinct instead of select). EHR_clean &lt;- EHR %&gt;% distinct(NPI, CCN, Provider_Type, Business_State_Territory, ZIP, Specialty, Hospital_Type, Vendor_Name, EHR_Product_Name, Product_Classification, Product_Setting) summary(EHR_clean) ## NPI CCN Provider_Type ## Min. :1.003e+09 Min. : 10001 EP :393375 ## 1st Qu.:1.255e+09 1st Qu.:121301 Hospital: 16330 ## Median :1.508e+09 Median :241361 ## Mean :1.501e+09 Mean :261394 ## 3rd Qu.:1.750e+09 3rd Qu.:390179 ## Max. :1.993e+09 Max. :670114 ## NA&#39;s :393375 ## Business_State_Territory ZIP ## California : 31692 : 16330 ## Minnesota : 24252 55905 : 11278 ## Florida : 22664 48109 : 3336 ## Texas : 22307 02114 : 3021 ## Pennsylvania: 20564 32224 : 2818 ## Michigan : 19739 60611 : 2521 ## (Other) :268487 (Other):370401 ## Specialty ## Family Medicine : 61485 ## Internal Medicine : 45820 ## Student in an Organized Health Care Education/Training Program: 17552 ## Cardiovascular Disease : 17524 ## : 16330 ## (Other) :250764 ## NA&#39;s : 230 ## Hospital_Type Vendor_Name ## :393375 Epic Systems Corporation:133637 ## Critical Access: 3348 Cerner Corporation : 70367 ## General : 12982 Allscripts : 28067 ## athenahealth, Inc. : 20970 ## eClinicalWorks, LLC : 16354 ## GE Healthcare : 14882 ## (Other) :125428 ## EHR_Product_Name ## EpicCare Ambulatory 2014 Certified EHR Suite: 71207 ## athenaClinicals : 20945 ## Beacon Oncology 2014 Certified Module : 16281 ## eClinicalWorks : 16097 ## NextGen Ambulatory EHR : 13616 ## Powerchart (Clinical) : 13293 ## (Other) :258266 ## Product_Classification Product_Setting ## : 14690 : 14690 ## Complete EHR:202827 Ambulatory:325708 ## Modular EHR :192188 Inpatient : 69307 ## ## ## ## Observations: There are 485841 records of missing CCN (the identifying number for eligible hospitals) and missing hospital type, which matches up with the number of records that are Provider_Type == 'EP'. It makes sense since these variables would not be relevant to an eligible professional (EP), who is not affiliated with any hospital. There are exactly 20073 records of missing zip codes and blank specialty (specifically not NA’s), and this number matches up with the number of Provider_Type == 'Hospital'. It sounds like the we just need to split the data into EPs and hospitals since there are enough distinct variables relevant for each type. Then clean each one before merging with their corresponding level of demographics. Split: # split for EPs EHR_EPs &lt;- EHR_clean %&gt;% filter(Provider_Type == &#39;EP&#39;) %&gt;% select(-CCN, -Hospital_Type, -Provider_Type) summary(EHR_EPs) ## NPI Business_State_Territory ZIP ## Min. :1.003e+09 California : 30425 55905 : 11278 ## 1st Qu.:1.255e+09 Minnesota : 23895 48109 : 3336 ## Median :1.508e+09 Florida : 21678 02114 : 3021 ## Mean :1.501e+09 Texas : 20941 32224 : 2818 ## 3rd Qu.:1.750e+09 Pennsylvania: 19919 60611 : 2521 ## Max. :1.993e+09 Michigan : 19221 23298 : 2235 ## (Other) :257296 (Other):368166 ## Specialty ## Family Medicine : 61485 ## Internal Medicine : 45820 ## Student in an Organized Health Care Education/Training Program: 17552 ## Cardiovascular Disease : 17524 ## Obstetrics &amp; Gynecology : 15059 ## (Other) :235705 ## NA&#39;s : 230 ## Vendor_Name ## Epic Systems Corporation:131883 ## Cerner Corporation : 64893 ## Allscripts : 27743 ## athenahealth, Inc. : 20929 ## eClinicalWorks, LLC : 16345 ## GE Healthcare : 14861 ## (Other) :116721 ## EHR_Product_Name ## EpicCare Ambulatory 2014 Certified EHR Suite: 71047 ## athenaClinicals : 20928 ## Beacon Oncology 2014 Certified Module : 16189 ## eClinicalWorks : 16097 ## NextGen Ambulatory EHR : 13613 ## Powerchart (Clinical) : 12548 ## (Other) :242953 ## Product_Classification Product_Setting ## : 14462 : 14462 ## Complete EHR:200889 Ambulatory:324643 ## Modular EHR :178024 Inpatient : 54270 ## ## ## ## # check for valid zip codes EHR_EPs %&gt;% filter(nchar(as.character(ZIP)) &lt; 5) %&gt;% group_by(ZIP) %&gt;% summarize(n()) ## # A tibble: 3 x 2 ## ZIP `n()` ## &lt;fctr&gt; &lt;int&gt; ## 1 1010 1 ## 2 EH16 2 ## 3 IP28 9 # split for hospitals EHR_hosps &lt;- EHR_clean %&gt;% filter(Provider_Type == &#39;Hospital&#39;) %&gt;% select(-ZIP, -Specialty, -Provider_Type) summary(EHR_hosps) ## NPI CCN Business_State_Territory ## Min. :1.003e+09 Min. : 10001 Texas : 1366 ## 1st Qu.:1.245e+09 1st Qu.:121301 California : 1267 ## Median :1.488e+09 Median :241361 Florida : 986 ## Mean :1.495e+09 Mean :261394 Illinois : 667 ## 3rd Qu.:1.740e+09 3rd Qu.:390179 Pennsylvania: 645 ## Max. :1.993e+09 Max. :670114 Ohio : 564 ## (Other) :10835 ## Hospital_Type ## : 0 ## Critical Access: 3348 ## General :12982 ## ## ## ## ## Vendor_Name ## Cerner Corporation :5474 ## Medical Information Technology, Inc. (MEDITECH):2879 ## Epic Systems Corporation :1754 ## MEDHOST : 653 ## McKesson : 625 ## Cerner Health Services, Inc. : 525 ## (Other) :4420 ## EHR_Product_Name ## HealthSentry : 851 ## Powerchart (Clinical) : 745 ## Powerchart (All CQM&#39;s) : 703 ## P2 Sentinel (Powered by Sensage) : 580 ## EpicCare Inpatient 2014 Certified EHR Suite: 565 ## HealtheLife and Powerchart : 552 ## (Other) :12334 ## Product_Classification Product_Setting ## : 228 : 228 ## Complete EHR: 1938 Ambulatory: 1065 ## Modular EHR :14164 Inpatient :15037 ## ## ## ## The summaries much look better separately for the two sets. Now we just have to clean up the zip codes for the physician-level EHR’s. Looks like there are only a few records with invalid zip codes. Similar to how we cleaned up the zip codes for EPs, we will match the zip codes with R’s own zip code package. # first remove non-US states, appending DC to list state_name &lt;- c(state.name,&quot;District Of Columbia&quot;) states &lt;- data.frame(state_name = state_name, state_abb = c(state.abb, &quot;DC&quot;)) EHR_EPs &lt;- EHR_EPs %&gt;% filter(Business_State_Territory %in% state_name) length(unique(EHR_EPs$Business_State_Territory)) ## [1] 51 # remove invalid zip codes EHR_EPs &lt;- EHR_EPs %&gt;% filter(nchar(as.character(ZIP)) &gt;= 5) # merge R zip codes library(zipcode) data(&quot;zipcode&quot;) EHR_EPs &lt;- EHR_EPs %&gt;% left_join(zipcode, by = c(&quot;ZIP&quot; = &quot;zip&quot;)) ## Warning: Column `ZIP`/`zip` joining factor and character vector, coercing ## into character vector # check state mismatches EHR_mismatch &lt;- EHR_EPs %&gt;% left_join(states, by = c(&quot;state&quot; = &quot;state_abb&quot;)) %&gt;% # need to add in long state names mutate(Business_State_Territory = as.character(Business_State_Territory)) %&gt;% filter(state_name != Business_State_Territory) ## Warning: Column `state`/`state_abb` joining character vector and factor, ## coercing into character vector # percent state mismatch nrow(EHR_mismatch)/nrow(EHR_EPs) ## [1] 0.04492693 # check which zips are mismatching EHR_mismatch %&gt;% group_by(ZIP, Business_State_Territory, state_name) %&gt;% summarize(num = n()) %&gt;% arrange(desc(num)) %&gt;% head(8) ## # A tibble: 8 x 4 ## # Groups: ZIP, Business_State_Territory [8] ## ZIP Business_State_Territory state_name num ## &lt;chr&gt; &lt;chr&gt; &lt;fctr&gt; &lt;int&gt; ## 1 51503 Nebraska Iowa 120 ## 2 99362 Washington Oregon 83 ## 3 54601 Minnesota Wisconsin 80 ## 4 19104 New Jersey Pennsylvania 65 ## 5 55905 Florida Minnesota 63 ## 6 33331 Ohio Florida 58 ## 7 85259 Minnesota Arizona 48 ## 8 55905 Wisconsin Minnesota 46 The top 10 mismatches: Zip code 51503 is in Iowa (agrees with zip code package)/bordering Nebraska, but reported as Nebraska which is neighboring Iowa. This may be correct, so we’ll keep Nebraska as reported Zip code 55905 is in Minnesota, but reported state Florida is not near Minnesota. Since there is likely a typo in either zip or state, we’ll remove this mismatched zip all together Zip code 54601 is in Wisconsin/bordering Minnesota, but reported as Minnesota which is neighboring Wisconsin. We’ll keep Minnesota as reported Zip code 33331 is in Florida, which is nowhere near Ohio. Since it is unclear which was the typo, we’ll delete all these mismatches Zip code 55905 is in Minnesota - found it’s actually Mayo Clinic (along with #2). Wisconsin and Minnesota are bordering each other, so we’ll keep Wisconsin –&gt; in general, if we look at test_mismatch %&gt;% filter(ZIP == &quot;55905&quot;) you will notice that this zip code is incorrectly used for multiple states, possibly meaning there were data entry/quality issues, which validates our decision to clean up the zip codes. EHR_zip &lt;- EHR_EPs %&gt;% left_join(states, by = c(&quot;state&quot; = &quot;state_abb&quot;)) %&gt;% mutate(Business_State_Territory = as.character(Business_State_Territory)) %&gt;% filter( (state_name == Business_State_Territory) | (state_name != Business_State_Territory &amp;&amp; ZIP == &quot;51503&quot; &amp;&amp; Business_State_Territory == &quot;Nebraska&quot;) | (state_name != Business_State_Territory &amp;&amp; ZIP == &quot;54601&quot; &amp;&amp; Business_State_Territory == &quot;Minnesota&quot;) | (state_name != Business_State_Territory &amp;&amp; ZIP == &quot;55905&quot; &amp;&amp; Business_State_Territory == &quot;Wisconsin&quot;) ) %&gt;% select(NPI, Business_State_Territory, ZIP, Specialty, Vendor_Name, EHR_Product_Name, Product_Classification, Product_Setting) ## Warning: Column `state`/`state_abb` joining character vector and factor, ## coercing into character vector Merge: We want to inner join with the EPs data set on NPI, which has one row per physician by their unique NPI (one-to-many merge) We want to inner join with the hosps data set on CCN, which has one row per hospital by their unique CCN (one-to-many merge) # merge with EPs data EHR_EPs &lt;- inner_join(EHR_zip, EPs, by=c(&#39;NPI&#39;=&#39;npi&#39;)) summary(EHR_EPs) ## NPI Business_State_Territory ZIP ## Min. : NA Length:0 Length:0 ## 1st Qu.: NA Class :character Class :character ## Median : NA Mode :character Mode :character ## Mean :NaN ## 3rd Qu.: NA ## Max. : NA ## ## Specialty ## :0 ## Acupuncturist :0 ## Acute Care :0 ## Addiction Medicine :0 ## Addiction Psychiatry:0 ## Adolescent Medicine :0 ## (Other) :0 ## Vendor_Name ## 4medica, Inc. :0 ## Abeo Solutions, Inc :0 ## ACL Laboratories :0 ## Acmeware, Inc. :0 ## ACOM Health, Division of ACOM Solutions, Inc.:0 ## Acrendo Software, Inc. :0 ## (Other) :0 ## EHR_Product_Name Product_Classification ## 1 Connect BuildYourEMR :0 :0 ## 2013 Systemedx Clinical Navigator :0 Complete EHR:0 ## 24/7 smartEMR :0 Modular EHR :0 ## 4medica iEHR Cloud Ambulatory Suite:0 ## A.I.med Pro :0 ## abcEHR :0 ## (Other) :0 ## Product_Setting cred cty ehr gndr grd_yr ## :0 NULL: 20850 :0 NULL: F:0 Min. : NA ## Ambulatory:0 ABBEVILLE :0 M:0 1st Qu.: NA ## Inpatient :0 ABBOTSFORD:0 U:0 Median : NA ## ABERDEEN :0 Mean :NaN ## ABERNATHY :0 3rd Qu.: NA ## ABILENE :0 Max. : NA ## (Other) :0 ## med_sch ## A T STILL UN, ARIZONA SCHL OF DENT.Y &amp; ORAL HLTH :0 ## ADIO/PENNSYLVANIA INSTITUTE OF STRAIGHT CHIROPRACTIC :0 ## ALBANY MEDICAL COLLEGE OF UNION UNIVERSITY :0 ## ALBERT EINSTEIN COLLEGE OF MEDICINE OF YESHIVA UNIVERSITY :0 ## AMERICAN MEDICAL MISSIONARY COLLEGE :0 ## ARIZONA COLLEGE OF OSTEOPATHIC MEDICINE MID WESTERN UNIVERSITY:0 ## (Other) :0 ## pri_spec st ## :0 Length:0 ## ADDICTION MEDICINE :0 Class :character ## ADVANCED HEART FAILURE AND TRANSPLANT CARDIOLOGY:0 Mode :character ## ALLERGY/IMMUNOLOGY :0 ## ANESTHESIOLOGY :0 ## ANESTHESIOLOGY ASSISTANT :0 ## (Other) :0 ## zip zip.ext latitude longitude ## Length:0 Length:0 Min. : NA Min. : NA ## Class :character Class :character 1st Qu.: NA 1st Qu.: NA ## Mode :character Mode :character Median : NA Median : NA ## Mean :NaN Mean :NaN ## 3rd Qu.: NA 3rd Qu.: NA ## Max. : NA Max. : NA ## ## yrs_grd locations ## Min. : NA Min. : NA ## 1st Qu.: NA 1st Qu.: NA ## Median : NA Median : NA ## Mean :NaN Mean :NaN ## 3rd Qu.: NA 3rd Qu.: NA ## Max. : NA Max. : NA ## # merge with hosps data EHR_hosps &lt;- inner_join(EHR_hosps, hosps, by=c(&#39;CCN&#39;=&#39;hosp_afl_1&#39;)) summary(EHR_hosps) ## NPI CCN Business_State_Territory ## Min. :1.003e+09 Min. : 10001 California: 286 ## 1st Qu.:1.225e+09 1st Qu.:100236 Florida : 248 ## Median :1.488e+09 Median :220100 Ohio : 165 ## Mean :1.489e+09 Mean :238469 Illinois : 155 ## 3rd Qu.:1.740e+09 3rd Qu.:360175 Georgia : 153 ## Max. :1.993e+09 Max. :670067 Texas : 146 ## (Other) :1954 ## Hospital_Type ## : 0 ## Critical Access: 0 ## General :3107 ## ## ## ## ## Vendor_Name ## Cerner Corporation :1164 ## Medical Information Technology, Inc. (MEDITECH): 429 ## Epic Systems Corporation : 351 ## Cerner Health Services, Inc. : 148 ## HCA Information Technology &amp; Services, Inc. : 136 ## McKesson : 134 ## (Other) : 745 ## EHR_Product_Name ## HealthSentry : 180 ## Powerchart (Clinical) : 152 ## Powerchart (All CQM&#39;s) : 143 ## P2 Sentinel (Powered by Sensage) : 121 ## EpicCare Inpatient 2014 Certified EHR Suite: 109 ## Powerchart (eRX) : 101 ## (Other) :2301 ## Product_Classification Product_Setting Hospital_name ## : 40 : 40 Length:3107 ## Complete EHR: 233 Ambulatory: 241 Class :character ## Modular EHR :2834 Inpatient :2826 Mode :character ## ## ## ## ## State Staffed_beds Total_discharges Patient_days ## Length:3107 Min. : 7.0 Min. : 12 Min. : 37 ## Class :character 1st Qu.: 140.0 1st Qu.: 5487 1st Qu.: 21839 ## Mode :character Median : 241.0 Median :10563 Median : 45289 ## Mean : 284.3 Mean :12719 Mean : 61001 ## 3rd Qu.: 368.0 3rd Qu.:17049 3rd Qu.: 81364 ## Max. :1750.0 Max. :77757 Max. :425114 ## ## Gross_patient_revenue num_phys female_prop avg_grad_year ## Min. : 3736 Min. : 1.000 Min. :0.0000 Min. :1965 ## 1st Qu.: 504866 1st Qu.: 1.000 1st Qu.:0.0000 1st Qu.:2004 ## Median : 920311 Median : 1.000 Median :0.5000 Median :2008 ## Mean : 1276255 Mean : 1.736 Mean :0.4839 Mean :2007 ## 3rd Qu.: 1636428 3rd Qu.: 2.000 3rd Qu.:1.0000 3rd Qu.:2011 ## Max. :14140692 Max. :12.000 Max. :1.0000 Max. :2016 ## NA&#39;s :9 ## n_specialty EHR_use ## Min. :1.000 1: 826 ## 1st Qu.:1.000 0:2281 ## Median :1.000 ## Mean :1.638 ## 3rd Qu.:2.000 ## Max. :8.000 ## # check EHR use (should expect all 1&#39;s or &quot;yes&quot;) table(EHR_EPs$ehr) ## &lt; table of extent 0 &gt; table(EHR_hosps$ehr) ## &lt; table of extent 0 &gt; Notice that the ehr column is not all 1’s or Y’s when we expect them to be. this explains why we had so many physicians who fell in the ehr='' category. the physicians whom we have EHR product usage information on did not answer “yes” to EHR use in the original EPs data. 4.3.3 Output saveRDS(EHR_EPs, file = &quot;./data/EHR_EPs.rds&quot;) saveRDS(EHR_hosps, file = &quot;./data/EHR_hosps.rds&quot;) Although the specific product types are too granular and not meaningful enough to clean, we believe that exploring the professional and hospital demographics across vendor types may still be interesting, especially if we focus on the top 10 most popular vendors or classify the smaller local vendors as one group. "],
["primary.html", "Chapter 5 Primary Analysis 5.1 Physician Demographics 5.2 Hospital Demographics", " Chapter 5 Primary Analysis 5.1 Physician Demographics Since practitioners who are affiliated with a hospital may not have a choice in using EHR or not, we will exclude these from our analysis population, which is now just the practitioners who enrolled in the Medicare Incentive Program who are not affiliated with any hospital, which come from the data set EPs that we cleaned in section 4.1. EPs &lt;- readRDS(&quot;./data/EPs.rds&quot;) We will use logistic regression because we have a dichotomous outcome (EHR used: Y/N) and want to explore the relationship between the outcome and other predictor/explanatory variables. The coefficients generated from logistic regression will give us a formula to predict a logit transformation of the probability of the outcome. The general formula will look like this: \\[ ln\\left(\\frac{p}{1-p}\\right) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ....+\\beta_kX_k \\] 5.1.1 Exploratory Before fitting our model, we explored the relationships between our variables of interest using Association Rule Learning. We will use the R package called arulesViz to help us visualize this because we have mostly categorical variables with too many levels for simple correlation matrices to handle. Here, each “transaction” is a practitioner who adapted EHR as part of the Medicare EHR Incentive Program in the U.S. library(&quot;arulesViz&quot;) ## Loading required package: arules ## ## Attaching package: &#39;arules&#39; ## The following object is masked from &#39;package:car&#39;: ## ## recode ## The following object is masked from &#39;package:dplyr&#39;: ## ## recode ## The following objects are masked from &#39;package:base&#39;: ## ## abbreviate, write ## Loading required package: grid # first need to keep certain associational variables of interest and discretize them corr &lt;- EPs %&gt;% ungroup() %&gt;% filter(med_sch != &quot;OTHER&quot;) %&gt;% select(gndr, grd_yr, pri_spec, st) %&gt;% mutate(grd_yr = as.factor(grd_yr), st = as.factor(st)) # convert from a data frame to a transaction dataset corrt &lt;- as(corr, &quot;transactions&quot;) # create rules using the apriori rules &lt;- apriori(corrt, parameter=list(support=0.01, confidence=0.5)) ## Apriori ## ## Parameter specification: ## confidence minval smax arem aval originalSupport maxtime support minlen ## 0.5 0.1 1 none FALSE TRUE 5 0.01 1 ## maxlen target ext ## 10 rules FALSE ## ## Algorithmic control: ## filter tree heap memopt load sort verbose ## 0.1 TRUE TRUE FALSE TRUE 2 TRUE ## ## Absolute minimum support count: 0 ## ## set item appearances ...[0 item(s)] done [0.00s]. ## set transactions ...[58 item(s), 50 transaction(s)] done [0.00s]. ## sorting and recoding items ... [58 item(s)] done [0.00s]. ## creating transaction tree ... done [0.00s]. ## checking subsets of size 1 2 3 4 done [0.00s]. ## writing ... [804 rule(s)] done [0.00s]. ## creating S4 object ... done [0.00s]. plot(rules) The result is a a set of ______________ association rules with generally high confidence and low support (proportion of transactions in the data set which contain the item set). Let’s first trim this down a bit to show only important rules (confidence &gt; 0.85). We’ll pick the top 30 rules so we have a smaller subset to find meaningful relationships. The top 30 rules are chosen with respect to the lift measure (a measure of rule strength) - the deviation of the support of the whole rule from the support expected under independence given the supports of both sides of the rule. subrules &lt;- rules[quality(rules)$confidence &gt; 0.85] inspect(head(sort(subrules, by =&quot;lift&quot;),30)) ## lhs rhs support confidence lift count ## [1] {st=CT} =&gt; {grd_yr=1990} 0.02 1 50 1 ## [2] {grd_yr=1990} =&gt; {st=CT} 0.02 1 50 1 ## [3] {pri_spec=UROLOGY} =&gt; {st=OR} 0.02 1 50 1 ## [4] {st=OR} =&gt; {pri_spec=UROLOGY} 0.02 1 50 1 ## [5] {pri_spec=CERTIFIED NURSE MIDWIFE} =&gt; {st=NC} 0.02 1 50 1 ## [6] {st=NC} =&gt; {pri_spec=CERTIFIED NURSE MIDWIFE} 0.02 1 50 1 ## [7] {grd_yr=1992} =&gt; {st=NY} 0.02 1 50 1 ## [8] {st=NY} =&gt; {grd_yr=1992} 0.02 1 50 1 ## [9] {pri_spec=PHYSICIAN ASSISTANT} =&gt; {st=AK} 0.02 1 50 1 ## [10] {st=AK} =&gt; {pri_spec=PHYSICIAN ASSISTANT} 0.02 1 50 1 ## [11] {pri_spec=OPTOMETRY, ## st=CT} =&gt; {grd_yr=1990} 0.02 1 50 1 ## [12] {grd_yr=1990, ## pri_spec=OPTOMETRY} =&gt; {st=CT} 0.02 1 50 1 ## [13] {gndr=F, ## st=CT} =&gt; {grd_yr=1990} 0.02 1 50 1 ## [14] {gndr=F, ## grd_yr=1990} =&gt; {st=CT} 0.02 1 50 1 ## [15] {grd_yr=1995, ## st=MD} =&gt; {pri_spec=INTERNAL MEDICINE} 0.02 1 50 1 ## [16] {grd_yr=2007, ## pri_spec=CARDIOVASCULAR DISEASE (CARDIOLOGY)} =&gt; {st=TN} 0.02 1 50 1 ## [17] {pri_spec=CHIROPRACTIC, ## st=CA} =&gt; {grd_yr=2000} 0.02 1 50 1 ## [18] {grd_yr=2004, ## pri_spec=UROLOGY} =&gt; {st=OR} 0.02 1 50 1 ## [19] {grd_yr=2004, ## st=OR} =&gt; {pri_spec=UROLOGY} 0.02 1 50 1 ## [20] {gndr=F, ## pri_spec=UROLOGY} =&gt; {st=OR} 0.02 1 50 1 ## [21] {gndr=F, ## st=OR} =&gt; {pri_spec=UROLOGY} 0.02 1 50 1 ## [22] {grd_yr=2003, ## pri_spec=PSYCHIATRY} =&gt; {st=UT} 0.02 1 50 1 ## [23] {gndr=M, ## pri_spec=PSYCHIATRY} =&gt; {st=UT} 0.02 1 50 1 ## [24] {gndr=M, ## grd_yr=2003} =&gt; {st=UT} 0.02 1 50 1 ## [25] {pri_spec=PHYSICAL THERAPY, ## st=TX} =&gt; {grd_yr=2002} 0.02 1 50 1 ## [26] {gndr=M, ## pri_spec=PHYSICAL THERAPY} =&gt; {grd_yr=2002} 0.02 1 50 1 ## [27] {grd_yr=1995, ## pri_spec=CERTIFIED NURSE MIDWIFE} =&gt; {st=NC} 0.02 1 50 1 ## [28] {grd_yr=1995, ## st=NC} =&gt; {pri_spec=CERTIFIED NURSE MIDWIFE} 0.02 1 50 1 ## [29] {gndr=F, ## pri_spec=CERTIFIED NURSE MIDWIFE} =&gt; {st=NC} 0.02 1 50 1 ## [30] {gndr=F, ## st=NC} =&gt; {pri_spec=CERTIFIED NURSE MIDWIFE} 0.02 1 50 1 plot(subrules, method=&quot;grouped&quot;, control=list(k=50)) ## Warning: did not converge in 50 iterations ## Warning: did not converge in 50 iterations ## Warning: did not converge in 50 iterations ## Warning: did not converge in 50 iterations ## Warning: did not converge in 50 iterations ## Warning: did not converge in 50 iterations ## Warning: did not converge in 50 iterations We concluded the following: Practitioner graduation year is not very interesting Medical School, Primary Specialty, and Gender had the most meaningful associations However, we would choose only one of medical school or primary specialty. They are likely highly correlated because there are specialty-specific schools such as chiropractic schools. Now we need to explore the relationships of our continuous independent variables Years since graduation by gender using jittered violin plots EPs %&gt;% melt(id.vars=&quot;gndr&quot;, measure.vars=&quot;yrs_grd&quot;) %&gt;% ggplot(aes(gndr, value)) + geom_jitter(alpha = 0.1) + geom_violin(alpha = 0.75) + facet_grid(variable ~ .) + scale_y_sqrt() Gender seems to be pretty independent of the number of years since graduation so we should be able to add both to the model without influencing each other’s effects. The distribution of years since graduation is skewed, so we used a square root scale to make the kernel density curves look more symmetric in the plots than it otherwise would have been. The actual values of the years since graduation were left alone so we could intuitively interpret the results from our model. Number of locations by gender using jittered violin plots EPs %&gt;% melt(id.vars=&quot;gndr&quot;, measure.vars=&quot;locations&quot;) %&gt;% ggplot(aes(gndr, value)) + geom_jitter(alpha = 0.1) + geom_violin(alpha = 0.75) + facet_grid(variable ~ .) + scale_y_log10() While practice locations seem to be distributed evenly between males and females, note that the large majority of physicians in our data set have only one location. There are a few outliers who have over 300 unique zip codes associated with their practices. Years since graduation by credentials using bubble plots EPs %&gt;% mutate(cred = reorder(cred, yrs_grd)) %&gt;% ggplot(aes(cred, yrs_grd)) + stat_sum(aes(size = ..n.., group = 1)) + scale_size_area(max_size = 10) Credentials (physician degrees) had one of the fewest number of levels, so we wanted to see if it was a good candidate for our model. The distribution of years since graduation looked pretty consistent across different credentials. Unfortunately, there were disproportionally high numbers of physicians with credentials listed as N/A (~75%), meaning their credential was unknown, so we could not use this variable in our model. Gender, years since graduation, and number of locations by EHR use # scatter plot matrix of all three effects by EHR use my_colors &lt;- brewer.pal(nlevels(as.factor(EPs$ehr)), &quot;Set1&quot;) ## Warning in brewer.pal(nlevels(as.factor(EPs$ehr)), &quot;Set1&quot;): minimal value for n is 3, returning requested palette with 3 different levels EPs %&gt;% mutate(gender = as.factor(as.integer(gndr))) %&gt;% scatterplotMatrix(~gender+yrs_grd+locations|ehr, data=., col=my_colors, smoother.args=list(col=&quot;grey&quot;), cex=1.5 , pch=c(15,16), legend.pos=&quot;center&quot;) # bubble plot of years since graduation by EHR use EPs %&gt;% ggplot(aes(ehr, yrs_grd)) + stat_sum(aes(size = ..n.., group = 1)) + scale_size_area(max_size = 10) # jittered violin plot of years since graduation by EHR use EPs %&gt;% melt(id.vars=&quot;ehr&quot;, measure.vars=&quot;yrs_grd&quot;) %&gt;% ggplot(aes(ehr, value)) + geom_jitter(alpha = 0.1) + geom_violin(alpha = 0.75) + facet_grid(variable ~ .) + scale_y_sqrt() Note that we forced gender, which is binary, into the scatter plot matrix so just ignore any noise between the values 1 and 2 on the scale for gender. Again, we can confirm that all three variables of interest (gender, years since graduation, and number of locations) are not strongly correlated with each other at all. We can safely add them into the final model without interaction terms. From the this scatter plot matrix, it is apparent that the distribution of gender and years since graduation differ by EHR use (as indicated by the red and blue colors, blinded here because we want to give you some suspense dun dun dun - but actually, we just couldn’t get the legend to not completely cover the density curves). A general observation from the bubble and violin plots is that there are proportionally more physicians in our data who have not used EHR. so we already have an imbalance in sample size between the two groups. But overall, our sample size is still large enough. 5.1.2 Final Analysis 5.1.2.1 Fit the Logistic Model Our final physician-level logistic regression model looked like this: \\[ logit(EHR) = \\beta_0 + \\beta_1(gender) + \\beta_2(years~since~grad) + \\beta_3(location) \\] # fit the model model &lt;- glm(ehr ~ gndr + yrs_grd + locations, data = EPs, family = binomial) summary(model) ## ## Call: ## glm(formula = ehr ~ gndr + yrs_grd + locations, family = binomial, ## data = EPs) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -0.5057 -0.4150 -0.3340 -0.2781 2.6724 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.74754 1.16108 -1.505 0.132 ## gndrM -0.59814 0.87293 -0.685 0.493 ## yrs_grd -0.02975 0.04447 -0.669 0.504 ## locations -0.24481 0.73391 -0.334 0.739 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 52.751 on 114 degrees of freedom ## Residual deviance: 51.429 on 111 degrees of freedom ## AIC: 59.429 ## ## Number of Fisher Scoring iterations: 7 #table of odds ratios with 95% CI (ORtab &lt;- exp(cbind(OR = coef(model), confint(model)))) ## Waiting for profiling to be done... ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## OR 2.5 % 97.5 % ## (Intercept) 0.1742023 0.02478448 4.149219 ## gndrM 0.5498310 0.07518962 2.761591 ## yrs_grd 0.9706908 0.87213789 1.044496 ## locations 0.7828536 0.05005992 1.401855 Holding years since graduation and locations at a fixed value, the odds of adopting EHR for males is 0.55 the odds of adopting EHR for females. Holding gender (male) and locations at a fixed value, the odds of adopting EHR has a -2.93% increase for each additional year since graduation. We did not find a statistically significant effect in the number of practice locations on the use of EHR, so this makes me feel better about collapsing the records by unique physicians and losing the specific location information like city, state, and zip code. If owning practices in various different locations had an effect on EHR use, then we would have needed to consider fitting a mixed effects model that takes into account the random effects of the different locations these physicians practice in, or consider a repeated measures analysis on the non-collapsed data where a physician can have repeated records for each unique location of their practice, etc. But since we lack statistical evidence for the number of practice locations to show an effect on EHR use, we have no reason to seek a better model to fit. 5.1.2.2 Predicted Probabilities We can look at the effects of varying years since graduation by gender while holding the number of practice locations constant at its sample mean on the outcome of EHR use with a ribbon plot of the predicted probabilities. # create a table of predicted probabilities varying the value of years since graduation and gender varyvals &lt;- with(EPs, data.frame(yrs_grd = rep(seq(from = min(yrs_grd), to = max(yrs_grd), length.out = 100), 2), locations = mean(locations), gndr = factor(rep(c(&#39;M&#39;,&#39;F&#39;), each = 100)))) pred &lt;- cbind(varyvals, predict(model, newdata = varyvals, type = &quot;link&quot;, se = TRUE)) pred &lt;- within(pred, { PredictedProb &lt;- plogis(fit) LL &lt;- plogis(fit - (1.96 * se.fit)) UL &lt;- plogis(fit + (1.96 * se.fit)) }) # ribbon plot ggplot(pred, aes(x = yrs_grd, y = PredictedProb)) + geom_ribbon(aes(ymin = LL, ymax = UL, fill = gndr), alpha = 0.2) + geom_line(aes(colour = gndr), size = 1) Well, how well does the model with these predictors fit compared to a null model? Let’s perform the likelihood ratio test using a chi-square test of {r} with(model, null.deviance - deviance) (the difference in deviance for the two models) with {r} with(model, df.null - df.residual) degrees of freedom on our observed data, which gives us the following p-value: with(model, pchisq(null.deviance - deviance, df.null - df.residual, lower.tail = FALSE)) ## [1] 0.7237737 Looks like our model did pretty well! 5.2 Hospital Demographics Now let’s focus on the other type of providers eligible for the Medicaid &amp; Medicare EHR Incentive Program: the hospitals. Recall that in this analysis population, we included only the physicians who are affiliated with any hospital and aggregated their demographics and EHR use at the hospital level. This is because the use of EHR would no longer depend on the physicians themselves, but rather the hospitals who decide to participate in the program. Since the aggregated physician demographics are not reliable demographic representations of the hospitals, as discussed in the Data chapter, we will only perform exploratory analysis on them and focus on the scraped hospital demographics (staffed beds, total discharges, patient days, gross patient revenue) for the final analysis. 5.2.1 Exploratory ggplot(hosps, aes(num_phys, colour = EHR_use)) + geom_density() + xlim(0, 500) Again, we wanted to show that our data on the physicians for each hospital are limited, and could affect our final analysis if we only subset to hospitals with sufficient physicians data in order to accurately include the aggregated physician demographics of the hospitals. my_colors &lt;- brewer.pal(nlevels(as.factor(hosps$EHR_use)), &quot;Set1&quot;) # explore aggregated physician demographics scatterplotMatrix(~num_phys+female_prop+avg_grad_year+n_specialty|EHR_use, data=hosps, col=my_colors , smoother.args=list(col=&quot;grey&quot;) , cex=1.5 , pch=c(15,16), legend.plot=FALSE) # explore scraped hospital demographics scatterplotMatrix(~Staffed_beds+Total_discharges+Patient_days+Gross_patient_revenue|EHR_use, data=hosps, col=my_colors , smoother.args=list(col=&quot;grey&quot;) , cex=1.5 , pch=c(15,16)) The first correlation matrix of all the aggregated physician demographics at the hospital level is basically garbage. The second correlation matrix shows us that all four hospital demographics are strongly correlated with each other, which does make sense, and follow the same distributions by EHR use. We will also log-transform these demographics to help normalize their seemingly skewed distributions. hosps &lt;- hosps %&gt;% mutate(staffed_beds_log = round(log(Staffed_beds),2), total_discharges_log = round(log(Total_discharges),2), patient_days_log = round(log(Patient_days),2), gross_patient_rev_log = round(log(Gross_patient_revenue),2) ) # check for normality for one of them qqnorm(hosps$gross_patient_rev_log) qqline(hosps$gross_patient_rev_log) Let’s look at the correlation plots again with the log-transformed values. scatterplotMatrix(~staffed_beds_log+total_discharges_log+patient_days_log+gross_patient_rev_log|EHR_use, data=hosps, col=my_colors , smoother.args=list(col=&quot;grey&quot;) , cex=1.5 , pch=c(15,16), legend.pos=&quot;bottomleft&quot;) hosps %&gt;% ggplot(aes(EHR_use, gross_patient_rev_log)) + stat_sum(aes(size = ..n.., group = 1)) + scale_size_area(max_size = 10) ## Warning: Removed 5 rows containing non-finite values (stat_sum). The difference in distributions is not as apparent by the use of EHR. We believed that the gross patient revenue, which may be implying overall hospital size, is a confounding factor that affects other predictor variables. We can also observe this in the strong correlation (above 0.8) between GPR and staffed beds, GPR and total discharge, and GPR and patient days. Thus, we will stratify on gross patient revenue. Let’s see if other predictor variables still have effect on EHR use proportion after stratifying on GPR. hosps$gpr_grp = cut(hosps$gross_patient_rev_log, quantile(hosps$gross_patient_rev_log, prob = seq(0, 1, .2), na.rm = TRUE), include.lowest = TRUE) filter_var = &quot;total_discharges_log&quot; #&quot;staffed_beds_log&quot; #&quot;patient_days_log&quot; hosps %&gt;% filter(!is.na(gpr_grp)) %&gt;% group_by(gpr_grp) %&gt;% do(tidy(glm(EHR_use ~ staffed_beds_log + total_discharges_log + patient_days_log, data = ., family=&quot;binomial&quot;), conf.int = TRUE)) %&gt;% filter(term==filter_var) ## # A tibble: 5 x 8 ## # Groups: gpr_grp [5] ## gpr_grp term estimate std.error statistic ## &lt;fctr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 [8.23,12.9] total_discharges_log 0.4995140 1.481251 0.3372244 ## 2 (12.9,13.5] total_discharges_log 1.1669966 1.405116 0.8305341 ## 3 (13.5,14] total_discharges_log 0.3595741 1.254598 0.2866050 ## 4 (14,14.5] total_discharges_log -0.7145513 1.587193 -0.4501983 ## 5 (14.5,16.5] total_discharges_log -4.2080280 1.622126 -2.5941441 ## # ... with 3 more variables: p.value &lt;dbl&gt;, conf.low &lt;dbl&gt;, ## # conf.high &lt;dbl&gt; We are only showing the statistical results on total discharge because it is the only demographic that has a different significance in effect on EHR use across the stratification of gross patient revenue, as evident by the p-values. So in our final model, we will use total discharge and gross patient revenue and their interaction term. 5.2.2 Final Analysis We are just running the models below to show you our process of choosing the best fitting model, by splitting our data into training and test sets. Train &lt;- createDataPartition(hosps$EHR_use, p=0.6, list=FALSE) training &lt;- hosps[Train, ] testing &lt;- hosps[-Train, ] # first model with all hosptial demographics glm1 &lt;- glm(EHR_use ~ gross_patient_rev_log + staffed_beds_log + total_discharges_log + patient_days_log +staffed_beds_log:gross_patient_rev_log + total_discharges_log:patient_days_log, data=training, family = &quot;binomial&quot;) summary(glm1) ## ## Call: ## glm(formula = EHR_use ~ gross_patient_rev_log + staffed_beds_log + ## total_discharges_log + patient_days_log + staffed_beds_log:gross_patient_rev_log + ## total_discharges_log:patient_days_log, family = &quot;binomial&quot;, ## data = training) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.0665 -1.2065 0.6865 0.8399 1.3232 ## ## Coefficients: ## Estimate Std. Error z value ## (Intercept) 0.14197 7.64263 0.019 ## gross_patient_rev_log 0.04456 0.93069 0.048 ## staffed_beds_log 1.23819 2.27627 0.544 ## total_discharges_log 0.60371 1.16002 0.520 ## patient_days_log -0.32333 1.14458 -0.282 ## gross_patient_rev_log:staffed_beds_log -0.07326 0.16902 -0.433 ## total_discharges_log:patient_days_log -0.03161 0.09993 -0.316 ## Pr(&gt;|z|) ## (Intercept) 0.985 ## gross_patient_rev_log 0.962 ## staffed_beds_log 0.586 ## total_discharges_log 0.603 ## patient_days_log 0.778 ## gross_patient_rev_log:staffed_beds_log 0.665 ## total_discharges_log:patient_days_log 0.752 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 503.35 on 423 degrees of freedom ## Residual deviance: 479.20 on 417 degrees of freedom ## (4 observations deleted due to missingness) ## AIC: 493.2 ## ## Number of Fisher Scoring iterations: 4 p_hat_logit &lt;- predict(glm1, newdata = testing, type=&quot;response&quot;) y_hat_logit &lt;- ifelse(p_hat_logit &gt; 0.5, 1, 0) confusionMatrix(data = y_hat_logit, reference = testing$EHR_use) ## Warning in confusionMatrix.default(data = y_hat_logit, reference = testing ## $EHR_use): Levels are not in the same order for reference and data. ## Refactoring data to match. ## Confusion Matrix and Statistics ## ## Reference ## Prediction 1 0 ## 1 70 198 ## 0 9 6 ## ## Accuracy : 0.2686 ## 95% CI : (0.2178, 0.3242) ## No Information Rate : 0.7208 ## P-Value [Acc &gt; NIR] : 1 ## ## Kappa : -0.0488 ## Mcnemar&#39;s Test P-Value : &lt;2e-16 ## ## Sensitivity : 0.88608 ## Specificity : 0.02941 ## Pos Pred Value : 0.26119 ## Neg Pred Value : 0.40000 ## Prevalence : 0.27915 ## Detection Rate : 0.24735 ## Detection Prevalence : 0.94700 ## Balanced Accuracy : 0.45774 ## ## &#39;Positive&#39; Class : 1 ## Because total discharge and patient days have very high correlation (0.99), pick out one out of the two - total discharge. Also, because we observed from correlation plots that staffed beds and the gross patient revenue are highly correlated, we add interaction variable to the model. glm2 &lt;- glm(EHR_use ~ gross_patient_rev_log + staffed_beds_log + gross_patient_rev_log*staffed_beds_log + total_discharges_log, data=training, family = &quot;binomial&quot;) summary(glm2) ## ## Call: ## glm(formula = EHR_use ~ gross_patient_rev_log + staffed_beds_log + ## gross_patient_rev_log * staffed_beds_log + total_discharges_log, ## family = &quot;binomial&quot;, data = training) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.0222 -1.2112 0.6812 0.8371 1.2839 ## ## Coefficients: ## Estimate Std. Error z value ## (Intercept) 0.2640 7.7041 0.034 ## gross_patient_rev_log 0.1624 0.6492 0.250 ## staffed_beds_log 1.5486 1.5062 1.028 ## total_discharges_log -0.2016 0.3732 -0.540 ## gross_patient_rev_log:staffed_beds_log -0.1079 0.1043 -1.034 ## Pr(&gt;|z|) ## (Intercept) 0.973 ## gross_patient_rev_log 0.802 ## staffed_beds_log 0.304 ## total_discharges_log 0.589 ## gross_patient_rev_log:staffed_beds_log 0.301 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 503.35 on 423 degrees of freedom ## Residual deviance: 479.92 on 419 degrees of freedom ## (4 observations deleted due to missingness) ## AIC: 489.92 ## ## Number of Fisher Scoring iterations: 4 This time, we will pick total discharge instead of the staffed beds. Then we will compare two models (glm2 vs. glm3) using chisquare test. glm3 &lt;- glm(EHR_use ~ gross_patient_rev_log + total_discharges_log + gross_patient_rev_log*total_discharges_log, data=training, family = &quot;binomial&quot;) summary(glm3) ## ## Call: ## glm(formula = EHR_use ~ gross_patient_rev_log + total_discharges_log + ## gross_patient_rev_log * total_discharges_log, family = &quot;binomial&quot;, ## data = training) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.0634 -1.2278 0.6867 0.8480 1.2484 ## ## Coefficients: ## Estimate Std. Error z value ## (Intercept) 2.09116 9.59333 0.218 ## gross_patient_rev_log 0.01684 0.79677 0.021 ## total_discharges_log 0.51281 1.00947 0.508 ## gross_patient_rev_log:total_discharges_log -0.04800 0.07716 -0.622 ## Pr(&gt;|z|) ## (Intercept) 0.827 ## gross_patient_rev_log 0.983 ## total_discharges_log 0.611 ## gross_patient_rev_log:total_discharges_log 0.534 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 503.35 on 423 degrees of freedom ## Residual deviance: 480.58 on 420 degrees of freedom ## (4 observations deleted due to missingness) ## AIC: 488.58 ## ## Number of Fisher Scoring iterations: 4 p_hat_logit &lt;- predict(glm3, newdata = testing, type=&quot;response&quot;) y_hat_logit &lt;- ifelse(p_hat_logit &gt; 0.5, 1, 0) confusionMatrix(data = y_hat_logit, reference = testing$EHR_use) ## Warning in confusionMatrix.default(data = y_hat_logit, reference = testing ## $EHR_use): Levels are not in the same order for reference and data. ## Refactoring data to match. ## Confusion Matrix and Statistics ## ## Reference ## Prediction 1 0 ## 1 71 200 ## 0 8 4 ## ## Accuracy : 0.265 ## 95% CI : (0.2145, 0.3205) ## No Information Rate : 0.7208 ## P-Value [Acc &gt; NIR] : 1 ## ## Kappa : -0.0468 ## Mcnemar&#39;s Test P-Value : &lt;2e-16 ## ## Sensitivity : 0.89873 ## Specificity : 0.01961 ## Pos Pred Value : 0.26199 ## Neg Pred Value : 0.33333 ## Prevalence : 0.27915 ## Detection Rate : 0.25088 ## Detection Prevalence : 0.95760 ## Balanced Accuracy : 0.45917 ## ## &#39;Positive&#39; Class : 1 ## #chisquare test H0(null model): glm2, H1(alternative model):glm3 anova(glm3, glm2, test=&quot;Chisq&quot;) ## Analysis of Deviance Table ## ## Model 1: EHR_use ~ gross_patient_rev_log + total_discharges_log + gross_patient_rev_log * ## total_discharges_log ## Model 2: EHR_use ~ gross_patient_rev_log + staffed_beds_log + gross_patient_rev_log * ## staffed_beds_log + total_discharges_log ## Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi) ## 1 420 480.58 ## 2 419 479.92 1 0.65934 0.4168 Because the anova test yields that glm3 is better, our final model is glm3 with two variables: total staffed beds and gross patient revenue. Our final physician-level logistic regression model looked like this: \\[ logit(EHR) = \\beta_0 + \\beta_1(staffed~beds) + \\beta_2(gross~patient~revenue) + \\beta_3(interaction) \\] Here is a table of odds ratios with 95% CI. exp(cbind(OR = coef(glm3), confint(glm3))) ## Waiting for profiling to be done... ## OR 2.5 % ## (Intercept) 8.0943298 7.439151e-07 ## gross_patient_rev_log 1.0169789 1.725513e-01 ## total_discharges_log 1.6699833 1.583736e-01 ## gross_patient_rev_log:total_discharges_log 0.9531293 8.344907e-01 ## 97.5 % ## (Intercept) 4.031635e+10 ## gross_patient_rev_log 4.160706e+00 ## total_discharges_log 9.147742e+00 ## gross_patient_rev_log:total_discharges_log 1.137181e+00 Interpretation of the final model: If the total discharge is equal at 5000, with the 10% increase in the revenue, the odds of using EHR is 2.92 times higher. Similarly, if the gross patient revenue is equal at 5000 and the discharged increases by 10%, the odds of using EHR is 2.95 times higher. Mathematically, if revenue increases 10% and discharge is the same, the odds of using EHR is the higher by \\(1.102\\cdot \\log(1.1)-0.04\\cdot \\log(1.1)\\cdot \\log(discharge)\\) times. If discharge increases by 10% while the revenue is the same, the odds of using EHR is higher by \\(1.2411\\cdot \\log(1.1) - 0.046\\cdot \\log(1.1)\\cdot \\log(revenue)\\) "],
["secondary.html", "Chapter 6 Secondary Analysis 6.1 By Geographical Regions 6.2 By EHR Vendors", " Chapter 6 Secondary Analysis 6.1 By Geographical Regions 6.1.1 Physician Demographics data(&quot;zipcode&quot;) data(&quot;fifty_states&quot;) states_map &lt;- map_data(&quot;state&quot;) ## ## Attaching package: &#39;maps&#39; ## The following object is masked from &#39;package:purrr&#39;: ## ## map EHR_EPs %&gt;% group_by(Business_State_Territory) %&gt;% summarize(count = n()) %&gt;% ggplot() + geom_map(aes(map_id = id), data = fifty_states, map = fifty_states, color=&quot;grey50&quot;, fill=&quot;grey90&quot;) + geom_map(aes(map_id = tolower(Business_State_Territory), fill = count), map = fifty_states, color=&quot;grey50&quot;) + coord_map() + scale_fill_gradientn(colors = brewer.pal(6, &quot;RdYlBu&quot;)) + expand_limits(x=states_map$long, y=states_map$lat) + theme_grey() + theme(axis.text = element_blank(), axis.line = element_blank(), axis.ticks = element_blank(), panel.border = element_blank(), panel.grid = element_blank(), axis.title = element_blank(), legend.position = &quot;bottom&quot;) + ggtitle(&quot;Number of Physicians in Dataset&quot;) # gender distribution EHR_EPs %&gt;% group_by(Business_State_Territory) %&gt;% summarize(fem = mean(gndr == &quot;F&quot;), male = mean(gndr == &quot;M&quot;)) %&gt;% ggplot() + geom_map(aes(map_id = id), data = fifty_states, map = fifty_states, color=&quot;grey50&quot;, fill=&quot;grey90&quot;) + geom_map(aes(map_id = tolower(Business_State_Territory), fill = male), map = fifty_states, color=&quot;grey50&quot;) + coord_map() + scale_fill_gradientn(colors = brewer.pal(6, &quot;RdYlBu&quot;)) + expand_limits(x=states_map$long, y=states_map$lat) + theme_grey() + theme(axis.text = element_blank(), axis.line = element_blank(), axis.ticks = element_blank(), panel.border = element_blank(), panel.grid = element_blank(), axis.title = element_blank(), legend.position = &quot;bottom&quot;) + ggtitle(&quot;Distribution of Male Physicians&quot;) age &lt;- EHR_EPs %&gt;% filter(!is.na(grd_yr)) %&gt;% group_by(Business_State_Territory) %&gt;% summarize(yrsgrad = mean(2016 - grd_yr)) age %&gt;% ggplot() + geom_map(aes(map_id = id), data = fifty_states, map = fifty_states, color=&quot;grey50&quot;, fill=&quot;grey90&quot;) + geom_map(aes(map_id = tolower(Business_State_Territory), fill = yrsgrad), map = fifty_states, color=&quot;grey50&quot;) + coord_map() + scale_fill_gradientn(colors = brewer.pal(6, &quot;RdYlBu&quot;)) + expand_limits(x=states_map$long, y=states_map$lat) + theme_grey() + theme(axis.text = element_blank(), axis.line = element_blank(), axis.ticks = element_blank(), panel.border = element_blank(), panel.grid = element_blank(), axis.title = element_blank(), legend.position = &quot;bottom&quot;) + ggtitle(&quot;Years Since Graduation&quot;) Among those Physicians in our Vendor data set, there were many more freshly graduated practitioners in Minnesota compared to the rest of the country. We interpreted this as being similar to an ‘age’ variable, since people who have 15 years since graduating &amp; getting their degrees are more likely to be younger than those who have been graduated for 26 years. mn &lt;- subset(states_map, region %in% c(&quot;minnesota&quot;)) zip_mapdata &lt;- EHR_EPs %&gt;% filter(!is.na(grd_yr) &amp; Business_State_Territory == &quot;Minnesota&quot;) %&gt;% group_by(ZIP) %&gt;% summarize(yrsgrad = mean(2016 - grd_yr)) %&gt;% left_join(zipcode, by = c(&quot;ZIP&quot; = &quot;zip&quot;)) #minnesota only # zip_mapdata %&gt;% # ggplot() + # geom_polygon(data=mn, aes(x=long, y=lat, group = group),colour=&quot;white&quot;, fill=&quot;grey70&quot; ) + # geom_point(aes(x=longitude, y=latitude, color = yrsgrad),size=2,alpha=.5) + # coord_fixed(1.5) + # theme_void() #zoomed in on minnesota zip_mapdata %&gt;% ggplot() + geom_polygon(data=states_map, aes(x=long, y=lat, group = group),colour=&quot;grey50&quot;, fill=&quot;white&quot;) + geom_point(aes(x=longitude, y=latitude, color = yrsgrad),size=2,alpha=.8) + scale_color_gradientn(colors=brewer.pal(8, &quot;Reds&quot;)) + coord_fixed(xlim = c(-88, -98), ylim = c(42, 50), ratio = 1.5) + theme_void() + theme(legend.title = element_blank()) Here we can see that most of the Physicians we have are clustered near Minneapolis. useehr &lt;- EPs %&gt;% group_by(st) %&gt;% summarize(num = n(), used = sum(ehr == 1)) %&gt;% mutate(pct = used/num) state_name &lt;- c(state.name,&quot;District Of Columbia&quot;) states &lt;- data.frame(state_name = state_name, state_abb = c(state.abb, &quot;DC&quot;)) useehr &lt;- useehr %&gt;% left_join(states, by = c(&quot;st&quot; = &quot;state_abb&quot;)) ## Warning: Column `st`/`state_abb` joining character vector and factor, ## coercing into character vector useehr %&gt;% ggplot() + geom_map(aes(map_id = id), data = fifty_states, map = fifty_states, color=&quot;grey50&quot;, fill=&quot;grey90&quot;) + geom_map(aes(map_id = tolower(state_name), fill = pct), map = fifty_states, color=&quot;grey50&quot;) + coord_map() + scale_fill_gradientn(colors = brewer.pal(6, &quot;RdYlBu&quot;)) + expand_limits(x=states_map$long, y=states_map$lat) + theme_gray() + theme(axis.text = element_blank(), axis.line = element_blank(), axis.ticks = element_blank(), panel.border = element_blank(), panel.grid = element_blank(), axis.title = element_blank(), legend.position = &quot;bottom&quot;) + ggtitle(&quot;Percent EHR Usage&quot;) In our dataset overall, we had more blank information on EHR usage than we did on confirmed EHR usage, which is why the percentages are low overall. However, it is interesting to note that Alaska and Vermont had extremely low EHR usage, while the Midwest tended to have relatively more adoption. Perhaps Alaska and Vermont have more rural environments, while many of the EHR vendors are based in the Midwest. #find what types of product settings exist levels(EHR_EPs$Product_Setting) ## [1] &quot;&quot; &quot;Ambulatory&quot; &quot;Inpatient&quot; #ambulatory vs inpatient numbers EHR_EPs %&gt;% filter(Product_Setting != &quot;&quot;) %&gt;% summarize(amb = sum(Product_Setting == &quot;Ambulatory&quot;), inp = sum(Product_Setting == &quot;Inpatient&quot;)) ## amb inp ## 1 0 0 prod_setting &lt;- EHR_EPs %&gt;% filter(Product_Setting != &quot;&quot;) %&gt;% group_by(Business_State_Territory) %&gt;% summarize(num = n(), amb = sum(Product_Setting == &quot;Ambulatory&quot;), inpat = sum(Product_Setting == &quot;Inpatient&quot;)) %&gt;% mutate(p_amb = amb/num, p_inpp = inpat/num) # table for overall percentages in merge dataset prod_setting_overall &lt;- EHR_EPs %&gt;% summarize(total = n(), amb = sum(Product_Setting == &quot;Ambulatory&quot;), inpat = sum(Product_Setting == &quot;Inpatient&quot;)) %&gt;% mutate(&quot;Ambulatory (Outpatient)&quot; = amb/total, Inpatient= inpat/total) %&gt;% mutate(Place = &quot;Overall&quot;) %&gt;% select(Place, &quot;Ambulatory (Outpatient)&quot;, Inpatient) prod_setting_overall %&gt;% kable Place Ambulatory (Outpatient) Inpatient Overall NaN NaN ambulatory &lt;- prod_setting %&gt;% ggplot() + geom_map(aes(map_id = id), data = fifty_states, map = fifty_states, color=&quot;grey50&quot;, fill=&quot;grey90&quot;) + geom_map(aes(map_id = tolower(Business_State_Territory), fill = p_amb), map = fifty_states, color=&quot;grey50&quot;) + coord_map() + scale_fill_gradientn(colors = brewer.pal(6, &quot;RdYlBu&quot;)) + expand_limits(x=states_map$long, y=states_map$lat) + theme_gray() + theme(axis.text = element_blank(), axis.line = element_blank(), axis.ticks = element_blank(), panel.border = element_blank(), panel.grid = element_blank(), axis.title = element_blank(), legend.position = &quot;bottom&quot;) + ggtitle(&quot;Product Setting = Ambulatory&quot;) inpatient &lt;- prod_setting %&gt;% ggplot() + geom_map(aes(map_id = id), data = fifty_states, map = fifty_states, color=&quot;grey50&quot;, fill=&quot;grey90&quot;) + geom_map(aes(map_id = tolower(Business_State_Territory), fill = p_inpp), map = fifty_states, color=&quot;grey50&quot;) + coord_map() + scale_fill_gradientn(colors = brewer.pal(6, &quot;RdYlBu&quot;)) + expand_limits(x=states_map$long, y=states_map$lat) + theme_gray() + theme(axis.text = element_blank(), axis.line = element_blank(), axis.ticks = element_blank(), panel.border = element_blank(), panel.grid = element_blank(), axis.title = element_blank(), legend.position = &quot;bottom&quot;) + ggtitle(&quot;Product Setting = Inpatient&quot;) grid.arrange(ambulatory, inpatient, nrow = 1) # california = outlier cali &lt;- EHR_EPs %&gt;% filter(Business_State_Territory == &quot;California&quot;) %&gt;% summarize(total = n(), amb = sum(Product_Setting == &quot;Ambulatory&quot;), inpat = sum(Product_Setting == &quot;Inpatient&quot;)) %&gt;% mutate(&quot;Ambulatory (Outpatient)&quot; = amb/total, Inpatient = inpat/total) %&gt;% select(&quot;Ambulatory (Outpatient)&quot;, Inpatient) prod_setting_overall &lt;- bind_rows(prod_setting_overall, data_frame(Place = &quot;California&quot;, &quot;Ambulatory (Outpatient)&quot; = cali$`Ambulatory (Outpatient)`, Inpatient = cali$Inpatient )) prod_setting_overall %&gt;% kable Place Ambulatory (Outpatient) Inpatient Overall NaN NaN California NaN NaN Product Setting refers to the type of practice setting that the EHR product is certified for. The possible settings are: Ambulatory: doctor’s offices, clinics, and other outpatient facilities Inpatient: hospitals and other long-term care facilities We found that most EHR products were certified for ambulatory settings, by far. This extreme preference in ambulatory EHR products is consistent across the country, except in California where there is still a slight preference for ambulatory EHR products (among those adopting EHR’s in the incentive program), but it is less than the rest of the country. # how many unique vendors length(unique(factor(EHR_EPs$Vendor_Name))) ## [1] 0 # find top 10 vendors top10 &lt;- EHR_EPs %&gt;% filter(!is.na(Vendor_Name)) %&gt;% group_by(Vendor_Name) %&gt;% summarize(num = n()) %&gt;% arrange(desc(num)) %&gt;% top_n(10) ## Selecting by num #unique vendors vendors &lt;- as.vector(top10$Vendor_Name) # distribution of top 10 vendors EHR_EPs %&gt;% filter(Vendor_Name %in% vendors) %&gt;% group_by(Vendor_Name) %&gt;% summarise(num = n()) %&gt;% ggplot(aes(x = reorder(Vendor_Name, num), y = num)) + geom_bar(stat=&#39;identity&#39;) + theme(axis.text.x = element_text(angle = 45, hjust = 1)) # percent usage of top 10 sum(top10$num)/nrow(EHR_EPs) ## [1] NaN pct_popvendors &lt;- EHR_EPs %&gt;% group_by(Business_State_Territory) %&gt;% summarize(num = n(), total_v = sum(Vendor_Name %in% vendors), opp = num - total_v) %&gt;% mutate(p_vend = total_v/num, u_vend = opp/num) pct_popvendors %&gt;% ggplot() + geom_map(aes(map_id = id), data = fifty_states, map = fifty_states, color=&quot;grey50&quot;, fill=&quot;grey90&quot;) + geom_map(aes(map_id = tolower(Business_State_Territory), fill = p_vend), map = fifty_states, color=&quot;grey50&quot;) + coord_map() + scale_fill_gradientn(colors = brewer.pal(6, &quot;RdYlBu&quot;)) + expand_limits(x=states_map$long, y=states_map$lat) + theme_gray() + theme(axis.text = element_blank(), axis.line = element_blank(), axis.ticks = element_blank(), panel.border = element_blank(), panel.grid = element_blank(), axis.title = element_blank(), legend.position = &quot;bottom&quot;) + ggtitle(&quot;Usage of Top 10 Vendors&quot;) We found that the majority of EHR products used were from the top 10 vendors. Interestingly, though, Minnesota had almost 100% usage of the top 10 vendors. This could potentially be because many of the big vendors are based in the Midwest (ie Epic, Mayo Clinic, Cerner, etc). #most common specialties in general EHR_EPs %&gt;% group_by(pri_spec) %&gt;% summarize(num = n()) %&gt;% arrange(desc(num)) ## # A tibble: 0 x 2 ## # ... with 2 variables: pri_spec &lt;fctr&gt;, num &lt;int&gt; # most common state -&gt; specialty specialty &lt;- EHR_EPs %&gt;% group_by(Business_State_Territory, pri_spec) %&gt;% summarize(num = n()) %&gt;% arrange(Business_State_Territory, desc(num)) #check similar specialties #EHR_EPs %&gt;% filter(grepl(&quot;PSYCH&quot;, pri_spec)) %&gt;% distinct(pri_spec) specialty %&gt;% ggplot(aes(Business_State_Territory, pri_spec)) + geom_tile(aes(fill = num)) When it came to looking at patterns between primary specialty and states, it was nearly impossible to find useful information. There were too many states and too many different specialties to try to find significant correlations between the levels. Additionally, we tried to find group similar specialties but it did not make the results much more interpretable. 6.1.2 Hospital Demographics #load hosp_all data &amp; add long state names state_name &lt;- c(state.name,&quot;District Of Columbia&quot;) states &lt;- data.frame(state_name = state_name, state_abb = c(state.abb, &quot;DC&quot;)) hosp_all &lt;- hosp_clean %&gt;% left_join(states, by = c(&quot;State&quot; = &quot;state_abb&quot;)) ## Warning: Column `State`/`state_abb` joining character vector and factor, ## coercing into character vector beds &lt;- hosp_all %&gt;% filter(!is.na(Staffed_beds)) %&gt;% group_by(state_name) %&gt;% summarize(avgbed = mean(Staffed_beds)) beds %&gt;% ggplot() + geom_map(aes(map_id = id), data = fifty_states, map = fifty_states, color=&quot;grey50&quot;, fill=&quot;grey90&quot;) + geom_map(aes(map_id = tolower(state_name), fill = avgbed), map = fifty_states, color=&quot;grey50&quot;) + coord_map() + scale_fill_gradientn(colors = brewer.pal(6, &quot;RdYlBu&quot;)) + expand_limits(x=states_map$long, y=states_map$lat) + theme_gray() + theme(axis.text = element_blank(), axis.line = element_blank(), axis.ticks = element_blank(), panel.border = element_blank(), panel.grid = element_blank(), axis.title = element_blank(), legend.position = &quot;bottom&quot;) + ggtitle(&quot;Average Hospital Beds per hospital&quot;) Vermont and Rhode Island had the lowest number of average hospital beds per hospital, which makes sense since they are small states. Connecticut and Florida had the highest average, suggesting that hospitals in that state tend to be bigger on average. # patient days hosp_all %&gt;% filter(!is.na(Patient_days)) %&gt;% group_by(state_name) %&gt;% summarize(patient = mean(Patient_days)) %&gt;% ggplot() + geom_map(aes(map_id = id), data = fifty_states, map = fifty_states, color=&quot;grey50&quot;, fill=&quot;grey90&quot;) + geom_map(aes(map_id = tolower(state_name), fill = patient), map = fifty_states, color=&quot;grey50&quot;) + coord_map() + scale_fill_gradientn(colors = brewer.pal(6, &quot;RdYlBu&quot;)) + expand_limits(x=states_map$long, y=states_map$lat) + theme_gray() + theme(axis.text = element_blank(), axis.line = element_blank(), axis.ticks = element_blank(), panel.border = element_blank(), panel.grid = element_blank(), axis.title = element_blank(), legend.position = &quot;bottom&quot;) + ggtitle(&quot;Across states, mean patient days per hospital&quot;) The distribution of average days at the hospital was mostly proportional to the distribution of average hospital beds from the previous map. The differences were: Florida had one of the highest averages for hospital beds (biggest hospitals) but did not have one of the highest average patient days (yay!) Massachusetts, which was slightly above average in the number of hospital beds, ended up being one of the states with the longest patient days on average. #total discharges hosp_all %&gt;% filter(!is.na(Total_discharges)) %&gt;% group_by(state_name) %&gt;% summarize(discharges = mean(Total_discharges)) %&gt;% ggplot() + geom_map(aes(map_id = id), data = fifty_states, map = fifty_states, color=&quot;grey50&quot;, fill=&quot;grey90&quot;) + geom_map(aes(map_id = tolower(state_name), fill = discharges), map = fifty_states, color=&quot;grey50&quot;) + coord_map() + scale_fill_gradientn(colors = brewer.pal(6, &quot;RdYlBu&quot;)) + expand_limits(x=states_map$long, y=states_map$lat) + theme_gray() + theme(axis.text = element_blank(), axis.line = element_blank(), axis.ticks = element_blank(), panel.border = element_blank(), panel.grid = element_blank(), axis.title = element_blank(), legend.position = &quot;bottom&quot;) + ggtitle(&quot;Across states, mean discharges days per hospital on average&quot;) The distribution of average discharges was not very interesting - it was similar to the previous maps. #avg revenue per discharge revenue &lt;- hosp_all %&gt;% filter(!is.na(Gross_patient_revenue) &amp; !is.na(Total_discharges)) %&gt;% mutate(per = Gross_patient_revenue/Total_discharges) %&gt;% group_by(State) %&gt;% summarize(rev = mean(per)) revenue %&gt;% ggplot() + geom_map(aes(map_id = id), data = fifty_states, map = fifty_states, color=&quot;grey50&quot;, fill=&quot;grey90&quot;) + geom_map(aes(map_id = tolower(state_name), fill = rev), map = fifty_states, color=&quot;grey50&quot;) + coord_map() + scale_fill_gradientn(colors = brewer.pal(6, &quot;RdYlBu&quot;)) + expand_limits(x=states_map$long, y=states_map$lat) + theme_gray() + theme(axis.text = element_blank(), axis.line = element_blank(), axis.ticks = element_blank(), panel.border = element_blank(), panel.grid = element_blank(), axis.title = element_blank(), legend.position = &quot;bottom&quot;) + ggtitle(&quot;Across states, average revenue per discharge&quot;) Looks like Montana has the highest average gross patient revenue per discharge. 6.1.3 Regional Conclusions The regional analyses served mostly exploratory/informational purposes. While we noticed many patterns regarding the Midwest, it is not clear if the patterns are due to association or causation. For example, we saw that the Midwestern states tended to have higher rates of EHR usage compared to the rest of the country. However, we also saw that many of the “Top 10” vendors in our data set were also based in the Midwest. There could have been confounding effects - our data set may not have been representative of the entire country - Healthit.gov cited over 75% of Physician EHR adoption by 2015, while our data set could only match about 20% EHR usage in 2016. We could have had a skewed sample of Physicians that happened to have more EHR adopter living in the Midwest. Also, the first plot showed that we did have strong representation in Physician counts in Minnesota and California, but not as much in the rest of the country. 6.2 By EHR Vendors 6.2.1 Physician Demographics Check the distributions of number of practice locations, states, products, and vendors among practitioners whom we have EHR usage information on. Counts of these distributions are the number of practitioners. #specialty vs year since graduation top_spec &lt;- EHR_EPs %&gt;% filter(!is.na(pri_spec)) %&gt;% group_by(pri_spec) %&gt;% summarize(num = n()) %&gt;% arrange(desc(num)) %&gt;% head(10) spec_list &lt;- c(&quot;OPTOMETRY&quot;, &quot;CHIROPRACTIC&quot;, &quot;DERMATOLOGY&quot;, &quot;OPHTHALMOLOGY&quot;, &quot;FAMILY PRACTICE&quot;, &quot;INTERNAL MEDICINE&quot;, &quot;OBSTETRICS/GYNECOLOGY&quot;, &quot;PODIATRY&quot;, &quot;PSYCHIATRY&quot;, &quot;NEUROLOGY&quot;) EHR_EPs %&gt;% filter(pri_spec %in% spec_list) %&gt;% mutate(yrsgrad = 2016 - grd_yr) %&gt;% ggplot(aes(x = pri_spec, y = yrsgrad)) + geom_boxplot() + theme(axis.text.x = element_text(angle = 45, hjust = 1)) #vendors vs year since graduation EHR_EPs %&gt;% filter(Vendor_Name %in% vendors) %&gt;% mutate(yrsgrad = 2016 - grd_yr) %&gt;% ggplot(aes(x = Vendor_Name, y = yrsgrad)) + geom_boxplot() + theme(axis.text.x = element_text(angle = 45, hjust = 1)) #top vendors vs top specialty EHR_EPs %&gt;% filter((pri_spec %in% spec_list) &amp; (Vendor_Name %in% vendors)) %&gt;% ggplot(aes(x = pri_spec, y = Vendor_Name)) + geom_jitter(alpha = 0.2, color = &quot;steelblue4&quot;) + theme_minimal() + theme(axis.text.x = element_text(angle = 45, hjust = 1), legend.position = &quot;none&quot;, axis.title = element_blank()) + ggtitle(&quot;Top 10 Primary Specialties versus Top 10 Vendors&quot;) Interesting conclusions from the boxplots: Ophthalmology had the highest median years since graduation, which makes sense that we found that our EHR users had more years since graduation than non-EHR users Mayo Clinic and GE Healthcare were used by the youngest practitioners, both having median years since graduation under 10 years. Interesting conclusions from the jitter plot: Integrated Practice Solutions, Inc, the second most popular vendor in our data set, is exclusively used by chiropractors. They also dominate the other vendors among the chiropractors. Eyefinity, Inc also has a specific audience, with only ophthalmologists and optometrists (eye doctors) using their products. Compulink Business Systems, Inc, is also very popular among the two eye specialties and their products seem to be targeted for a specific audience. While NextGen Healthcare is used by all the top 10 specialties, they are most popular among the ophthalmologists and optometrists. Modernizing Medicine Inc is very popular among dermatologists. Epic Systems, Cerner Corporation, athenahealth, and GE Healthcare are pretty consistently used across all the top 10 disciplines. Epic is the most consistently popular, however. Explore practitioner demographics: gender, graduation year, medical school attended, primary specialty. The following demographic relationships are among physicians not affiliated with any hospitals whom we have EHR vendor and product information on. So the sub-population here includes only the physicians who do use EHR. EHR_EPs %&gt;% group_by(grd_yr) %&gt;% select(products) %&gt;% table() EHR_EPs %&gt;% ggplot(aes(factor(products), grd_yr)) + geom_boxplot() + facet_grid(.~gndr) + theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.25)) + xlab(&quot;Number of EHR Products&quot;) + ylab(&quot;Graduation Year&quot;) + ggtitle(&quot;Dist of Grad Year by Number of EHR Products Used by gndr&quot;) Perform Association Rule Learning on all variables from the first two sections. check the validity of using this method if my variables are not independent from one another. Here, each “transaction” is a practitioner who adapted EHR as part of the Medicare EHR Incentive Program in the U.S. ***used because we have variables with a lot of variables library(&quot;arulesViz&quot;) # need to keep certain associational variables of interest and discretize them assoc &lt;- EPs_EHR %&gt;% ungroup() %&gt;% select(-NPI, -First.Name, -Last.Name, -Used.electronic.health.records, -EHRuse) %&gt;% mutate(Gradyr = as.factor(grd_yr), State = as.factor(st), locations = as.factor(locations), products = as.factor(products), vendors = as.factor(vendors) ) %&gt;% select(-grd_yr, -st) View(assoc) # convert from a data frame to a transaction dataset assoctrans &lt;- as(assoc, &quot;transactions&quot;) # create rules using the apriori rules &lt;- apriori(assoctrans, parameter=list(support=0.01, confidence=0.5)) plot(rules) The result is a set of 4152 association rules with generally high confidence and low support (proportion of transactions in the dataset which contain the itemset). Let’s first trim this down a bit to show only important rules (confidence &gt; 0.85). We’ll pick the top 30 rules so we have a smaller subset to find meaningful relationships. The top 30 rules are chosen with respect to the lift measure (a measure of rule strength) - the deviation of the support of the whole rule from the support expected under independence given the supports of both sides of the rule. subrules &lt;- rules[quality(rules)$confidence &gt; 0.85] inspect(head(sort(subrules, by =&quot;lift&quot;),30)) plot(subrules, method=&quot;grouped&quot;, control=list(k=50)) The top 18 rules show that the strongest associations are among practitioners who use 5 EHR products across 2 different vendors in the state of Minnesota within a single practice. The second highest set of rules shows a strong association among practitioners who use 7 EHR products across 2 different vendors in the state of California within a single practice. To explore the top rules, we created a couple visualizations. We can visualize these top 18 rules using a directed network graph where the items are vertices and the directed edges are the rules in the direction of the antecedent (IF) to the consequent (THEN). (check this?) top18 &lt;- head(sort(subrules, by =&quot;lift&quot;),18) plot(top18, method=&quot;graph&quot;) # uses items and rules as vertices connecting them with directed edges plot(top18, method=&quot;graph&quot;, control=list(type=&quot;itemsets&quot;)) # uses itemsets as vertices and rules are represented by directed edges In a parallel coordinates plot, the width of the arrows represents support and the intensity of the color represent confidence. We plotted the top 18 rules. plot(top18, method=&quot;paracoord&quot;, control=list(reorder=TRUE)) Using Associative Rule Learning, we found the following factors were important: number of EHR products used number of vendors used region (states) "]
]
