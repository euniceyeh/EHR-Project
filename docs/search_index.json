[
["index.html", "BST 260 Final Project: EHR Chapter 1 Overview and Motivation", " BST 260 Final Project: EHR Eunice Yeh, Lauren Yoo, Katherine Wang 2017-12-09 Chapter 1 Overview and Motivation Provide an overview of the project goals and the motivation for it. Consider that this will be read by people who did not see your project proposal. "],
["work.html", "Chapter 2 Related Work", " Chapter 2 Related Work Anything that inspired you, such as a paper, a web site, or something we discussed in class. "],
["questions.html", "Chapter 3 Initial Questions", " Chapter 3 Initial Questions What questions are you trying to answer? How did these questions evolve over the course of the project? What new questions did you consider in the course of your analysis? What are the demographics of eligible physicians in the incentive program who have been using EHR? What are the demographics of eligible hospitals in the incentive program who have been using EHR? What are the demographics of both types of providers by the specific EHR vendors or products used? Answer all of the above questions again, but stratify geographically (city, state, or zip code). "],
["data.html", "Chapter 4 Data 4.1 Eligible Professionals (EPs) 4.2 Eligible Hospitals (Hosp) 4.3 Vendors &amp; Products (EHR)", " Chapter 4 Data All of our data are based on the 2016 Medicare &amp; Medicaid EHR Incentive Program since the adoption of EHR is an ongoing basis: a provider who adopted and used EHR during the 2014 program, for example, is likely to still be in the program the following years. So theorethically, the 2016 program would be the most cumulative source of providers adopting EHR. As mentioned in Related Work, the adoption of EHR has been consistently rising over the years across the U.S., so there is also no interest in looking at the use over time when we can just look at the latest collection of data. And since 2017 isn’t quite over yet, it wouldn’t be fair to start looking at the usage of EHR within the 2017 program, so the most recent completed data would be on the 2016 program. 4.1 Eligible Professionals (EPs) To answer the first of our Initial Questions as to whether or not a physician’s age, education, practice location, etc. may be related to their use of EHR in the incentive program, we must be able to find demographics data at the practitioner level. 4.1.1 Source/Metadata And here we are: readily available on Data.Medicare.gov are the demographics of physicians eligible for the Medicare &amp; Medicaid EHR incentive program at the practitioner level. We used the api link to read in the dataset and applied SQL filtering options to speed up the download since it is a large file. we specified the limit to be exactly 2254703 because that’s the exact number of rows the data contain, if we don’t specify this the default limit of rows that get downloaded is 100. to compensate, we will specify specific columns to download, which also be specified in the download link (looked thru metadata on the website to select variables i’m interested in). # change limit to 2254703 on the final run dat &lt;- read.csv(&quot;https://data.medicare.gov/resource/c8qv-268j.csv?$limit=10000&amp;$select=npi,frst_nm,gndr,cred,med_sch,grd_yr,pri_spec,cty,st,zip,hosp_afl_1,hosp_afl_lbn_1,ehr&amp;$order=npi&quot;) 4.1.2 Wrangling Let’s first take a look at the data structure and a summary of all the columns. str(dat) ## &#39;data.frame&#39;: 10000 obs. of 13 variables: ## $ cred : Factor w/ 16 levels &quot;&quot;,&quot;AU&quot;,&quot;CNA&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ## $ cty : Factor w/ 2023 levels &quot;ABBEVILLE&quot;,&quot;ABERDEEN&quot;,..: 454 56 1548 524 1785 217 1093 1093 74 74 ... ## $ ehr : Factor w/ 2 levels &quot;&quot;,&quot;Y&quot;: 1 1 1 1 2 1 1 1 1 1 ... ## $ frst_nm : Factor w/ 1872 levels &quot;AAMIR&quot;,&quot;AARON&quot;,..: 155 155 155 1722 1408 409 763 763 917 917 ... ## $ gndr : Factor w/ 2 levels &quot;F&quot;,&quot;M&quot;: 2 2 2 2 2 2 1 1 2 2 ... ## $ grd_yr : int 1994 1994 1994 2003 1999 2003 2007 2007 1997 1997 ... ## $ hosp_afl_1 : int 490126 490126 490126 140010 360262 391312 360098 360098 60024 60024 ... ## $ hosp_afl_lbn_1: Factor w/ 1450 levels &quot;&quot;,&quot;ABBEVILLE AREA MEDICAL CENTER&quot;,..: 615 615 615 341 743 919 584 584 1333 1333 ... ## $ med_sch : Factor w/ 210 levels &quot;A T STILL UN, ARIZONA SCHL OF DENT.Y &amp; ORAL HLTH&quot;,..: 101 101 101 160 101 108 128 128 97 97 ... ## $ npi : int 1003000126 1003000126 1003000126 1003000134 1003000142 1003000407 1003000423 1003000423 1003000480 1003000480 ... ## $ pri_spec : Factor w/ 72 levels &quot;ADDICTION MEDICINE&quot;,..: 33 33 33 53 3 21 44 44 24 24 ... ## $ st : Factor w/ 53 levels &quot;AK&quot;,&quot;AL&quot;,&quot;AR&quot;,..: 47 47 47 15 36 39 36 36 6 6 ... ## $ zip : int 201662247 222053610 241537474 602011718 436082675 158251367 440608702 440608702 800452545 800452545 ... summary(dat) ## cred cty ehr frst_nm gndr ## :9575 PITTSBURGH : 209 :8623 MICHAEL : 182 F:5699 ## MD : 253 HOUSTON : 156 Y:1377 SARAH : 134 M:4301 ## PA : 39 NEW YORK : 125 JENNIFER: 127 ## PT : 29 ANN ARBOR : 119 DAVID : 103 ## NP : 19 DALLAS : 117 MATTHEW : 92 ## OD : 17 PHILADELPHIA: 99 JOHN : 89 ## (Other): 68 (Other) :9175 (Other) :9273 ## grd_yr hosp_afl_1 ## Min. :1963 Min. : 10001 ## 1st Qu.:2004 1st Qu.:140011 ## Median :2009 Median :260032 ## Mean :2007 Mean :269085 ## 3rd Qu.:2012 3rd Qu.:390147 ## Max. :2017 Max. :800001 ## NA&#39;s :9 NA&#39;s :3156 ## hosp_afl_lbn_1 ## :3171 ## UNIVERSITY OF VIRGINIA MEDICAL CENTER : 64 ## BANNER PAYSON MEDICAL CENTER : 63 ## UNIVERSITY OF MICHIGAN HEALTH SYSTEM : 60 ## PROVIDENCE REGIONAL MEDICAL CENTER EVERETT: 59 ## EASTERN MAINE MEDICAL CENTER : 56 ## (Other) :6527 ## med_sch npi ## OTHER :6302 Min. :1.003e+09 ## BAYLOR COLLEGE OF MEDICINE : 73 1st Qu.:1.003e+09 ## UNIVERSITY OF FLORIDA COLLEGE OF MEDICINE: 67 Median :1.003e+09 ## LAKE ERIE COLLEGE OF OSTEOPATHIC MEDICINE: 63 Mean :1.003e+09 ## NEW YORK UNIVERSITY SCHOOL OF MEDICINE : 59 3rd Qu.:1.003e+09 ## INDIANA UNIVERSITY SCHOOL OF MEDICINE : 58 Max. :1.004e+09 ## (Other) :3378 ## pri_spec st ## NURSE PRACTITIONER :1569 TX : 780 ## PHYSICIAN ASSISTANT :1053 CA : 768 ## INTERNAL MEDICINE : 936 PA : 679 ## PHYSICAL THERAPY : 652 FL : 612 ## FAMILY PRACTICE : 638 NY : 592 ## CERTIFIED REGISTERED NURSE ANESTHETIST: 541 MI : 422 ## (Other) :4611 (Other):6147 ## zip ## Min. : 603 ## 1st Qu.:212086417 ## Median :452192364 ## Mean :476032054 ## 3rd Qu.:752434115 ## Max. :998017863 ## From the above outputs, we learned that: There are 53 levels of states, which means this data include states outside of the 50 mainland states and DC. we don’t have sufficient data on the virgin islands and guam, so we will have to filter these states out. There is one person whose gender is unknown, we will filter this person out since we want to group by gender. There are rows with missing graduation years, will need to filer this out too if graduation years is one of the main demographics of interest (will mess up plotting). Also, one or more physician who graduated in 2017 slipped into our data…we will ignore this by filtering it out too. The zip code, 603, is out of range. a standard zip code has to be at least 5 digits. looks like most zip codes in this data are the full 9-digit zip codes, which can be split into the standard 5-digit format and the 4-digit after. There are way too many levels of medical schools and primary specialties, we will probably focus on looking at the most popular ones for exploratory analysis. data(&quot;state&quot;) clean &lt;- dat %&gt;% filter(st %in% c(state.abb, &quot;DC&quot;)) %&gt;% filter(gndr %in% c(&quot;F&quot;, &quot;M&quot;) &amp; !is.na(as.integer(grd_yr)) &amp; grd_yr != 2017) Now we only have 51 levels of states, 2 levels of gender, and no records with missing graduation year. But we still have zip codes as small as 2138, which is still out of range. To make sure only keep valid zip codes (since there may be other ways the zip codes are not clean), we can try to use R’s zip code package to validate our zip codes. # separate out the first five zip codes from the last four extension to match R zipcode data clean &lt;- clean %&gt;% mutate(zip.ext = substr(as.character(zip), 6, 9), zip = substr(as.character(zip), 1, 5)) # use R zipcode package and data library(zipcode) data(&quot;zipcode&quot;) zip_dat &lt;- clean %&gt;% left_join(zipcode,by=&#39;zip&#39;) # zip_dat %&gt;% select(npi, zip, zip.ext, cty, st, city, state, latitude, longitude) %&gt;% filter(as.character(st) != state | is.na(as.integer(state))) %&gt;% arrange(as.integer(zip)) %&gt;% head(.,20) # test &lt;- zip_dat %&gt;% select(npi, zip, zip.ext, cty, st, city, state, latitude, longitude) %&gt;% mutate(st = as.character(st)) %&gt;% filter(st != state) %&gt;% arrange(as.integer(zip)) # check if any of the invalid zip codes were matched to R&#39;s zip code package zip_dat %&gt;% filter(nchar(zip) &lt; 5) %&gt;% select(npi, frst_nm, zip, cty, st, city, state) %&gt;% head(20) ## npi frst_nm zip cty st city state ## 1 1003003088 AVANI 8824 KENDALL PARK NJ &lt;NA&gt; &lt;NA&gt; ## 2 1003021866 DONNA 8060 MOUNT HOLLY NJ &lt;NA&gt; &lt;NA&gt; ## 3 1003024142 ALLISON 7719 WALL NJ &lt;NA&gt; &lt;NA&gt; ## 4 1003033390 BRIAN 6385 WATERFORD CT &lt;NA&gt; &lt;NA&gt; ## 5 1003033390 BRIAN 6385 WATERFORD CT &lt;NA&gt; &lt;NA&gt; ## 6 1003041427 LAURIE 8302 BRIDGETON NJ &lt;NA&gt; &lt;NA&gt; ## 7 1003149451 COURTNEY 8401 ATLANTIC CITY NJ &lt;NA&gt; &lt;NA&gt; ## 8 1003149477 ASHLEY 7430 MAHWAH NJ &lt;NA&gt; &lt;NA&gt; ## 9 1003150137 ALAN 2601 HYANNIS MA &lt;NA&gt; &lt;NA&gt; ## 10 1003162132 JESSICA 4901 WATERVILLE ME &lt;NA&gt; &lt;NA&gt; ## 11 1003188954 KIMBERLY 2557 OAK BLUFFS MA &lt;NA&gt; &lt;NA&gt; ## 12 1003228479 LAUREN 3103 MANCHESTER NH &lt;NA&gt; &lt;NA&gt; ## 13 1003248204 KELLY 2138 CAMBRIDGE MA &lt;NA&gt; &lt;NA&gt; As expected, none of the zip codes with only 4 digits matched, so these 13 records will have to be thrown away in order for us to be able to accurately do analysis on the location variables and regional analysis and to match with the vendor data by zip code. zip_mismatch &lt;- zip_dat %&gt;% mutate(st = as.character(st)) %&gt;% filter(st != state | nchar(zip) &lt; 5) %&gt;% select(npi, frst_nm, zip, cty, city, st, state, latitude, longitude) # check number of records mismatched nrow(zip_mismatch) ## [1] 399 # look at the top five mismatches (top5_mismatch &lt;- zip_mismatch %&gt;% group_by(zip, cty, st, city, state) %&gt;% summarize(n = n()) %&gt;% arrange(desc(n)) %&gt;% head(5)) ## # A tibble: 5 x 6 ## # Groups: zip, cty, st, city [5] ## zip cty st city state n ## &lt;chr&gt; &lt;fctr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 21156 BOSTON MA Upper Falls MD 10 ## 2 16550 WORCESTER MA Erie PA 7 ## 3 22152 BOSTON MA Springfield VA 7 ## 4 23602 PLYMOUTH MA Newport News VA 7 ## 5 31033 MANCHESTER NH Haddock GA 7 In order to not just blatantly throw away 399 mismatches, we will look at the top 5 most common mismatches on a case-by-case basis to try to salvage as many records as possible without searching too deeply in our dataset. Also, starting from the 6th most common mismatches, the number of records (n) were down to 58 or less, so they would have little effect on the dataset. The results of searching up the top 10 mismatches on Google Maps: the zipcode 99362 spans across Walla Walla in both WA and OR, so it is likely that the practice is located on the WA side of the zipcode coverage. We will ignore the state mismatch from the zipcode package for this case. the zipcode 52761 does belong to Muscatine, IA, which is right on the border of IL. Again, the practice is most likely located on the IA side. Will ignore the state mismatch for this case as well. the zip code for Granger, IN was most likely mistyped. The correct zip code for Granger is 46530, which is very close to 43530. We will fix the zipcode for this particular case only. the zipcode 22401 does belong to Frederickburg, VA, and is not near the border of VA and MD at all. There is a “Frederick” city in MD, but none of its zipcodes look similar enough to 22401. No clear culprit, will have to let this one go. :( The zipcode for Hanover, MD is 21076, which was most likely mistyped as 20176. We will fix this particular case as well. # leave out the 22401 Fredericksburg MD mismatch, fix the zip codes for Granger and Hanover. zip_fix &lt;- top5_mismatch %&gt;% ungroup() %&gt;% filter(cty != &#39;FREDERICKSBURG&#39;) %&gt;% mutate(zip = replace(zip, zip %in% c(43530, 20176), c(46530, 21076))) %&gt;% select(-n) mismatch_tokeep &lt;- zip_dat %&gt;% select(-zip) %&gt;% # we want to replace the original wrong zip codes with the fixed zip codes inner_join(zip_fix, by = c(&#39;cty&#39;,&#39;st&#39;, &#39;city&#39;, &#39;state&#39;)) ## Warning: Column `st` joining factor and character vector, coercing into ## character vector clean &lt;- zip_dat %&gt;% mutate(st = as.character(st), state = as.character(state)) %&gt;% filter(st == state | nchar(zip) == 5) %&gt;% # keeping only the matches bind_rows(mismatch_tokeep) # then add in the mismatches we wanted to keep or have fixed Continuing on with our wrangling… About 31.56% of the (original) data are of physicians not affiliated with any hospital, these are the practitioners we will subset on to do our practitioner-level analysis on since these are the only cases when a physician’s demographics can have an effect on the choice of using EHR. For physicians who are affiliated with a hospital, their demographics would no longer be informative as to whether or not they use EHR, it would then have to be the demographics of the hospitals that could inform the use of EHR at the hospital-level. Thus, we were able to find hospital demographics such as ___ on hospitals across the U.S. in general, which will be discussed in the next section within this chapter, but there’s no indication of participation in the incentive program nor EHR use, so we will have to merge that hospital demographic data with the practitioners in this dataset who are affiliated with any one hospital. for the practitioner-level analysis we want to: Calculate the number of years since graudation Use 1’s and 0’s instead of ‘Y’ and blanks as indicator of EHR use for our binary outcome Keep only one record per physician, which means we would have to lose the distinct practice locations for physicians who have multiple practices. But since we can’t fit our logistic model with either city, state, or zip code, there’s no point in keeping this detail. Instead, we can calculate the number of distinct practices these physicians have in order to collapse the data to the practitioner level. EPs &lt;- clean %&gt;% filter(is.na(as.integer(hosp_afl_1))) %&gt;% mutate(yrs_grd = 2016 - as.integer(grd_yr), # calc the n of years since graduation cred = case_when(cred == &#39;&#39; ~ &#39;NA&#39;, # assign blanks to NA&#39;s TRUE ~ as.character(cred)), cred = as.factor(cred), ehr = case_when(ehr == &#39;Y&#39; ~ 1, # use numeric indicators ehr == &#39;&#39; ~ 0), ehr = as.factor(ehr)) %&gt;% select(-hosp_afl_1, -hosp_afl_lbn_1, -frst_nm) %&gt;% distinct(npi, .keep_all = TRUE) # keep only one row per physician # calculate the number of practices for each physician SL &lt;- dat %&gt;% group_by(npi) %&gt;% summarise(locations = n_distinct(zip)) EPs &lt;- left_join(EPs, SL, by=&quot;npi&quot;) rm(SL) summary(EPs) ## cred cty ehr gndr grd_yr ## NA :1535 NEW YORK : 21 0:1531 F:1068 Min. :1965 ## PT : 14 ROCHESTER : 16 1: 70 M: 533 1st Qu.:2003 ## DC : 10 DALLAS : 14 Median :2009 ## MD : 10 COLUMBUS : 13 Mean :2006 ## CSW : 8 DURHAM : 13 3rd Qu.:2013 ## OD : 7 SAN ANTONIO: 13 Max. :2016 ## (Other): 17 (Other) :1511 ## med_sch npi ## OTHER :1133 Min. :1.003e+09 ## LOGAN COLLEGE OF CHIROPRACTIC : 19 1st Qu.:1.003e+09 ## SOUTHERN COLLEGE OF OPTOMETRY : 17 Median :1.003e+09 ## PALMER COLLEGE CHIROPRACTIC - DAVENPORT : 16 Mean :1.003e+09 ## NEW YORK CHIROPRACTIC COLLEGE : 12 3rd Qu.:1.003e+09 ## ILLINOIS COLLEGE OF OPTOMETRY AT CHICAGO: 11 Max. :1.004e+09 ## (Other) : 393 ## pri_spec st zip ## PHYSICAL THERAPY :306 Length:1601 Length:1601 ## NURSE PRACTITIONER :247 Class :character Class :character ## CLINICAL SOCIAL WORKER:201 Mode :character Mode :character ## CHIROPRACTIC :128 ## PHYSICIAN ASSISTANT :108 ## OPTOMETRY : 99 ## (Other) :512 ## zip.ext city state latitude ## Length:1601 Length:1601 Length:1601 Min. :20.04 ## Class :character Class :character Class :character 1st Qu.:33.87 ## Mode :character Mode :character Mode :character Median :38.69 ## Mean :37.66 ## 3rd Qu.:41.28 ## Max. :64.84 ## NA&#39;s :66 ## longitude yrs_grd locations ## Min. :-161.88 Min. : 0.00 Min. : 1.000 ## 1st Qu.:-102.09 1st Qu.: 3.00 1st Qu.: 1.000 ## Median : -88.16 Median : 7.00 Median : 1.000 ## Mean : -93.17 Mean :10.01 Mean : 1.314 ## 3rd Qu.: -80.70 3rd Qu.:13.00 3rd Qu.: 1.000 ## Max. : -73.07 Max. :51.00 Max. :25.000 ## NA&#39;s :66 [explain the analysis-ready data] 4.1.3 Output saveRDS(EPs, file = &quot;./data/EPs.rds&quot;) # don&#39;t actually need to save hosp_afl data, going to merge with hospital data soon # saveRDS(hosp_afl, file = &quot;./data/hosp_afl.RDS&quot;) hosp_afl &lt;- dat %&gt;% filter(!is.na(as.integer(hosp_afl_1))) EPs data will be used to analyze the potential effects of practitioner demographics on the use of EHR, contains one row per physician hosp_afl is to be merged with the hospital demographics data discussed in the next section, contains one row per physician per state both are to be merged with the EHR vendor and product information to explore these demographics by specific types of EHR used. 4.2 Eligible Hospitals (Hosp) General hospital demographics such as the total number of staffed beds, total number of discharges, total number of patient days, and total gross patient revenue (inpatient and outpatient) are all publicly accessible on the American Hospital Directory organized by U.S. states. Since there is not a .csv file ready for us to download and the hospital demographics are already neatly summarized in a table for each state, we decided that we could do some web scraping for this. The details of these hospital statistics are described here. 4.2.1 Source/Metadata The link to each U.S. state’s summary table of hospital demographics by hospital name in that state begins with the same URL: https://www.ahd.com/states/hospital_ and ends with the state’s abbreviation plus a .html. For example, a table of hospital demographics for each hospital in Massachusetts can be directly scraped from the link, https://www.ahd.com/states/hospital_MA.html. # code for scraping a = 1 b = 2 a + b # check that this does not evaluate to anything str(hosp_web) ## &#39;data.frame&#39;: 3334 obs. of 7 variables: ## $ Hospital_name : chr &quot;Alaska Regional Hospital&quot; &quot;Bartlett Regional Hospital&quot; &quot;Central Peninsula General Hospital&quot; &quot;Fairbanks Memorial Hospital&quot; ... ## $ City : chr &quot;Anchorage&quot; &quot;Juneau&quot; &quot;Soldotna&quot; &quot;Fairbanks&quot; ... ## $ Staffed_beds : num 174 57 106 212 74 379 125 88 71 49 ... ## $ Total_discharges : num 6405 1710 2575 4513 4857 ... ## $ Patient_days : num 33507 6325 10186 21706 19498 ... ## $ Gross_patient_revenue: num 947754 147231 305469 456240 596437 ... ## $ State : chr &quot;ak_t&quot; &quot;ak_t&quot; &quot;ak_t&quot; &quot;ak_t&quot; ... 4.2.2 Wrangling looks like there are some hospitals with really low values - turns out they indicate missing data with zeroes, so we need to convert all zeroes to missing so that the zero values do not misinform summary statistics, and we highly doubt that the zeros are actually meaningfully zeros…right? merge in with hosp_afl, which is a subset of practitioners who are affiliated with a hospital, on the hospital name and city (since different hospitals in different locations can have the same name). 4.2.3 Output hosp will be used to analyze the potential effects of hospital demographics on the use of EHR and to subsequently merge with the EHR vendor and product information to explore these demographics by specific types of EHR used. 4.3 Vendors &amp; Products (EHR) Since so many professionals and hospitals across the U.S. have already adopted EHR, we realized that simplying looking at the binary outcome of EHR use would be too boring of a project. Thus, we were curious to further explore the demographics of those who do use EHR by the specific types of EHR vendor or product. Fortunately, we were able to find data to support this additional secondary/exploratory analysis. 4.3.1 Source/Metadata The Health IT Dashboard provides certified health IT product data from the ONC Certified Health IT Product List (CHPL) such as the unique vendors, products, and product types of each certified health IT product used as part of the Medicare EHR Incentive Program. We downloaded only the 2016 dataset, which also includes unique provider identifiers (NPI), in order to match the EPs dataset discussed in the first section of this chapter. As the metadata explains, a provider in this dataset can be either an eligible professional (EP) and eligible hospital (Hospital), as distinguished by the Provider_Type column. Thus, only the Provider_Type == 'EP' records is merged with the subset of EPs who are not affiliated with any hospital, and the Provider_Type == 'Hospital' records is merged with the subset of EPs who are affiliated with a hospital, and therefore directly merged with the combined and cleaned version of the hosp dataset discussed in the previous section of this chapter. EHR &lt;- read.csv(&quot;https://dashboard.healthit.gov/datadashboard/data/MU_REPORT_2016.csv&quot;) 4.3.2 Wrangling specific product types are too dirty and not meaningful, will focus on vendor types. merge with the subset of EPs who are not affiliated with any hospital, basically just the cleaned EPs notice that the ehr column is not all y when we expect them to be. this explains why we had so many physicians who fell in the ehr='' category. the physicians whom we have EHR product usage information on did not answer “yes” to EHR use in the phys data. again the top primary specialties overall are summary(dat$pri_spec), compare that to the top specialities among physicians whom we have vendor information: summary(vend_EPs$pri_spec). merge with the cleaned hosp data 4.3.3 Output Although the specific product types are too granular and not meaningful enough to clean, we believe that exploring the professional and hospital demographics across vendor types may still be interesting, especially if we focus on the top 10 most popular vendors or classify the smaller local vendors as one group. "],
["primary.html", "Chapter 5 Primary Analysis 5.1 Physician Demographics 5.2 Hospital Demographics", " Chapter 5 Primary Analysis For the exploratory sections, we aim to answer the following objectives: What visualizations did you use to look at your data in different ways? What are the different statistical methods you considered? Justify the decisions you made, and show any major changes to your ideas. How did you reach these conclusions? For the final analysis sections, we focuses on: What did you learn about the data? How did you answer the questions? How can you justify your answers? 5.1 Physician Demographics Since practitioners who are affiliated with a hospital may not have a choice in using EHR or not, we will exclude these from our analysis population, which is now just the practitioners who enrolled in the Medicare Incentive Program who are not affiliated with any hospital, which come from the data set EPs that we cleaned in section 4.1. EPs &lt;- readRDS(&quot;./data/EPs.rds&quot;) [explain that we are doing logistic regression model because of our binary outcome EHR use, and write out the mathematical theory and methods here.] 5.1.1 Exploratory Before fitting our model, we explored the relationships between our variables of interest using Association Rule Learning. We will use the R package called arulesViz to help us visualize this because we have mostly categorical variables with too many levels for simple correlation matrices to handle. Here, each “transaction” is a practitioner who adapted EHR as part of the Medicare EHR Incentive Program in the U.S. library(&quot;arulesViz&quot;) ## Loading required package: arules ## ## Attaching package: &#39;arules&#39; ## The following object is masked from &#39;package:car&#39;: ## ## recode ## The following object is masked from &#39;package:dplyr&#39;: ## ## recode ## The following objects are masked from &#39;package:base&#39;: ## ## abbreviate, write ## Loading required package: grid # first need to keep certain associational variables of interest and discretize them corr &lt;- EPs %&gt;% ungroup() %&gt;% filter(med_sch != &quot;OTHER&quot;) %&gt;% select(gndr, grd_yr, pri_spec, st) %&gt;% mutate(grd_yr = as.factor(grd_yr), st = as.factor(st)) # convert from a data frame to a transaction dataset corrt &lt;- as(corr, &quot;transactions&quot;) # create rules using the apriori rules &lt;- apriori(corrt, parameter=list(support=0.01, confidence=0.5)) ## Apriori ## ## Parameter specification: ## confidence minval smax arem aval originalSupport maxtime support minlen ## 0.5 0.1 1 none FALSE TRUE 5 0.01 1 ## maxlen target ext ## 10 rules FALSE ## ## Algorithmic control: ## filter tree heap memopt load sort verbose ## 0.1 TRUE TRUE FALSE TRUE 2 TRUE ## ## Absolute minimum support count: 4 ## ## set item appearances ...[0 item(s)] done [0.00s]. ## set transactions ...[145 item(s), 468 transaction(s)] done [0.00s]. ## sorting and recoding items ... [70 item(s)] done [0.00s]. ## creating transaction tree ... done [0.00s]. ## checking subsets of size 1 2 3 done [0.00s]. ## writing ... [85 rule(s)] done [0.00s]. ## creating S4 object ... done [0.00s]. plot(rules) The result is a a set of ______________ association rules with generally high confidence and low support (proportion of transactions in the data set which contain the item set). Let’s first trim this down a bit to show only important rules (confidence &gt; 0.85). We’ll pick the top 30 rules so we have a smaller subset to find meaningful relationships. The top 30 rules are chosen with respect to the lift measure (a measure of rule strength) - the deviation of the support of the whole rule from the support expected under independence given the supports of both sides of the rule. subrules &lt;- rules[quality(rules)$confidence &gt; 0.85] inspect(head(sort(subrules, by =&quot;lift&quot;),30)) ## lhs rhs support confidence lift count ## [1] {st=OK} =&gt; {gndr=M} 0.01068376 1.0000000 2.197183 5 ## [2] {grd_yr=2012, ## pri_spec=CHIROPRACTIC} =&gt; {gndr=M} 0.01282051 1.0000000 2.197183 6 ## [3] {pri_spec=DIAGNOSTIC RADIOLOGY} =&gt; {gndr=M} 0.01282051 0.8571429 1.883300 6 ## [4] {grd_yr=2007, ## pri_spec=OPTOMETRY} =&gt; {gndr=M} 0.01282051 0.8571429 1.883300 6 ## [5] {grd_yr=2007, ## pri_spec=NURSE PRACTITIONER} =&gt; {gndr=F} 0.01068376 1.0000000 1.835294 5 ## [6] {pri_spec=NURSE PRACTITIONER} =&gt; {gndr=F} 0.05555556 0.9629630 1.767320 26 ## [7] {pri_spec=PEDIATRIC MEDICINE} =&gt; {gndr=F} 0.01709402 0.8888889 1.631373 8 plot(subrules, method=&quot;grouped&quot;, control=list(k=50)) We concluded the following: Practitioner graduation year is not very interesting Medical School, Primary Specialty, and Gender had the most meaningful associations However, we would choose only one of medical school or primary specialty. They are likely highly correlated because there are specialty-specific schools such as chiropractic schools. Now we need to explore the relationships of our continuous independent variables Years since graduation by gender using jittered violin plots EPs %&gt;% melt(id.vars=&quot;gndr&quot;, measure.vars=&quot;yrs_grd&quot;) %&gt;% ggplot(aes(gndr, value)) + geom_jitter(alpha = 0.1) + geom_violin(alpha = 0.75) + facet_grid(variable ~ .) + scale_y_sqrt() Gender seems to be pretty independent of the number of years since graduation so we should be able to add both to the model without influencing each other’s effects. The distribution of years since graduation is skewed, so we used a square root scale to make the kernel density curves look more symmetric in the plots than it otherwise would have been. The actual values of the years since graduation were left alone so we could intuitively interpret the results from our model. Number of locations by gender using jittered violin plots EPs %&gt;% melt(id.vars=&quot;gndr&quot;, measure.vars=&quot;locations&quot;) %&gt;% ggplot(aes(gndr, value)) + geom_jitter(alpha = 0.1) + geom_violin(alpha = 0.75) + facet_grid(variable ~ .) + scale_y_log10() While practice locations seem to be distributed evenly between males and females, note that the large majority of physicians in our data set have only one location. There are a few outliers who have over 300 unique zip codes associated with their practices. Years since graduation by credentials using bubble plots EPs %&gt;% mutate(cred = reorder(cred, yrs_grd)) %&gt;% ggplot(aes(cred, yrs_grd)) + stat_sum(aes(size = ..n.., group = 1)) + scale_size_area(max_size = 10) Credentials (physician degrees) had one of the fewest number of levels, so we wanted to see if it was a good candidate for our model. The distribution of years since graduation looked pretty consistent across different credentials. Unfortunately, there were disproportionally high numbers of physicians with credentials listed as N/A (~75%), meaning their credential was unknown, so we could not use this variable in our model. Gender, years since graduation, and number of locations by EHR use # scatter plot matrix of all three effects by EHR use my_colors &lt;- brewer.pal(nlevels(as.factor(EPs$ehr)), &quot;Set1&quot;) ## Warning in brewer.pal(nlevels(as.factor(EPs$ehr)), &quot;Set1&quot;): minimal value for n is 3, returning requested palette with 3 different levels EPs %&gt;% mutate(gender = as.factor(as.integer(gndr))) %&gt;% scatterplotMatrix(~gender+yrs_grd+locations|ehr, data=., col=my_colors, smoother.args=list(col=&quot;grey&quot;), cex=1.5 , pch=c(15,16), legend.plot=FALSE) # bubble plot of years since graduation by EHR use EPs %&gt;% ggplot(aes(ehr, yrs_grd)) + stat_sum(aes(size = ..n.., group = 1)) + scale_size_area(max_size = 10) # jittered violin plot of years since graduation by EHR use EPs %&gt;% melt(id.vars=&quot;ehr&quot;, measure.vars=&quot;yrs_grd&quot;) %&gt;% ggplot(aes(ehr, value)) + geom_jitter(alpha = 0.1) + geom_violin(alpha = 0.75) + facet_grid(variable ~ .) + scale_y_sqrt() Note that we forced gender, which is binary, into the scatter plot matrix so just ignore any noise between the values 1 and 2 on the scale for gender. From the this scatter plot matrix, it is apparent that the distribution of gender and years since graduation differ by EHR use (as indicated by the red and blue colors, blinded here because we want to give you some suspense dun dun dun - but actually, we just couldn’t get the legend to not completely cover the density curves). A general observation from the bubble and violin plots is that there are proportionally more physicians in our data who have not used EHR. so we already have an imbalance in sample size between the two groups. But overall, our sample size is still large enough. 5.1.2 Final Analysis 5.1.2.1 Fit the Logistic Model [write out our final model] # fit the model model &lt;- glm(ehr ~ gndr + yrs_grd + locations, data = EPs, family = binomial) summary(model) ## ## Call: ## glm(formula = ehr ~ gndr + yrs_grd + locations, family = binomial, ## data = EPs) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -0.5328 -0.3522 -0.2465 -0.2338 2.7075 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -3.66972 0.23914 -15.345 &lt; 2e-16 *** ## gndrM 0.85466 0.24895 3.433 0.000597 *** ## yrs_grd 0.01783 0.01031 1.730 0.083652 . ## locations 0.01255 0.08678 0.145 0.885043 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 575.08 on 1600 degrees of freedom ## Residual deviance: 558.65 on 1597 degrees of freedom ## AIC: 566.65 ## ## Number of Fisher Scoring iterations: 6 #table of odds ratios with 95% CI exp(cbind(OR = coef(model), confint(model))) ## Waiting for profiling to be done... ## OR 2.5 % 97.5 % ## (Intercept) 0.02548362 0.01585523 0.04123045 ## gndrM 2.35056990 1.44398896 3.84687839 ## yrs_grd 1.01798938 0.99676093 1.03804917 ## locations 1.01262573 0.79323722 1.15495640 5.1.2.2 Predicted Probabilities We can look at the effects of varying years since graduation by gender while holding the number of practice locations constant at the mean on the outcome of EHR use with a ribbon plot of the predicted probabilities. # create a table of predicted probabilities varying the value of years since graduation and gender varyvals &lt;- with(EPs, data.frame(yrs_grd = rep(seq(from = min(yrs_grd), to = max(yrs_grd), length.out = 100), 2), locations = mean(locations), gndr = factor(rep(c(&#39;M&#39;,&#39;F&#39;), each = 100)))) pred &lt;- cbind(varyvals, predict(model, newdata = varyvals, type = &quot;link&quot;, se = TRUE)) pred &lt;- within(pred, { PredictedProb &lt;- plogis(fit) LL &lt;- plogis(fit - (1.96 * se.fit)) UL &lt;- plogis(fit + (1.96 * se.fit)) }) # ribbon plot ggplot(pred, aes(x = yrs_grd, y = PredictedProb)) + geom_ribbon(aes(ymin = LL, ymax = UL, fill = gndr), alpha = 0.2) + geom_line(aes(colour = gndr), size = 1) Well, how well does the model with these predictors fit compared to a null model? Let’s perform the likelihood ratio test using a chi-square test of {r} with(model, null.deviance - deviance) (the difference in deviance for the two models) with {r} with(model, df.null - df.residual) degrees of freedom on our observed data, which gives us the following p-value: with(model, pchisq(null.deviance - deviance, df.null - df.residual, lower.tail = FALSE)) ## [1] 0.0009280001 5.2 Hospital Demographics Describe the analysis population Exploratory: Show some correlations higher revenue, higher EHR use higher staffed beds, higher EHR use State the model Run the model &amp; describe the results Conclusion/discussion "],
["secondary.html", "Chapter 6 Secondary Analysis 6.1 EHR Vendor Types 6.2 Exploring the Characteristics of Non-hospital-affiliated Practitioners Who Use EHR 6.3 Regional", " Chapter 6 Secondary Analysis 6.1 EHR Vendor Types Merging of the vendor and practitioner datasets more clean up exploratory results 6.2 Exploring the Characteristics of Non-hospital-affiliated Practitioners Who Use EHR Check the distributions of number of practice locations, states, products, and vendors among practitioners whom we have EHR usage information on. Counts of these distributions are the number of practitioners. Explore practitioner demographics: gender, graduation year, medical school attended, primary specialty. merged_EPs_vendor %&gt;% group_by(Graduation.year) %&gt;% select(products) %&gt;% table() ** ADD LATER: show N’s at each point of the x-axis ** phys_EHR %&gt;% ggplot(aes(factor(products), Graduation.year)) + geom_boxplot() + facet_grid(.~Gender) + theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.25)) + xlab(&quot;Number of EHR Products&quot;) + ylab(&quot;Graduation Year&quot;) + ggtitle(&quot;Dist of Grad Year by Number of EHR Products Used by Gender&quot;) Perform Association Rule Learning on all variables from the first two sections. check the validity of using this method if my variables are not independent from one another. Here, each “transaction” is a practitioner who adapted EHR as part of the Medicare EHR Incentive Program in the U.S. ***used becuse we have variables with a lot of variables library(&quot;arulesViz&quot;) # need to keep certain associational variables of interest and discretize them assoc &lt;- phys_EHR %&gt;% ungroup() %&gt;% select(-NPI, -First.Name, -Last.Name, -Used.electronic.health.records, -EHRuse) %&gt;% mutate(Gradyr = as.factor(Graduation.year), State = as.factor(state_abb), locations = as.factor(locations), products = as.factor(products), vendors = as.factor(vendors) ) %&gt;% select(-Graduation.year, -state_abb) View(assoc) # convert from a data frame to a transaction dataset assoctrans &lt;- as(assoc, &quot;transactions&quot;) # create rules using the apriori rules &lt;- apriori(assoctrans, parameter=list(support=0.01, confidence=0.5)) plot(rules) The result is a set of 4152 association rules with generally high confidence and low support (proportion of transactions in the dataset which contain the itemset). Let’s first trim this down a bit to show only important rules (confidence &gt; 0.85). We’ll pick the top 30 rules so we have a smaller subset to find meaningful relationships. The top 30 rules are chosen with respect to the lift measure (a measure of rule strength) - the deviation of the support of the whole rule from the support expected under independence given the supports of both sides of the rule. subrules &lt;- rules[quality(rules)$confidence &gt; 0.85] inspect(head(sort(subrules, by =&quot;lift&quot;),30)) plot(subrules, method=&quot;grouped&quot;, control=list(k=50)) The top 18 rules show that the strongest associations are among practitioners who use 5 EHR products across 2 different vendors in the state of Minnesota within a single practice. The second highest set of rules shows a strong association among practitioners who use 7 EHR products across 2 different vendors in the state of California within a single practice. To explore the top rules, we created a couple visualizations. We can visualize these top 18 rules using a directed network graph where the items are vertices and the directed edges are the rules in the direction of the antecedent (IF) to the consequent (THEN). (check this?) top18 &lt;- head(sort(subrules, by =&quot;lift&quot;),18) plot(top18, method=&quot;graph&quot;) # uses items and rules as vertices connecting them with directed edges plot(top18, method=&quot;graph&quot;, control=list(type=&quot;itemsets&quot;)) # uses itemsets as vertices and rules are represented by directed edges In a parallel coordinates plot, the width of the arrows represents support and the intensity of the color represent confidence. We plotted the top 18 rules. plot(top18, method=&quot;paracoord&quot;, control=list(reorder=TRUE)) Using Associative Rule Learning, we found the following factors were important: number of EHR products used number of vendors used region (states) The results from the physician-vendor data was not that helpful, so we decided not to use it in our final model. conclusions - how do these inform our final analyses? 6.3 Regional physician demographics across the US hospital demographics across the US "]
]
